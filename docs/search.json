[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "pages/linreg.html",
    "href": "pages/linreg.html",
    "title": "Simple Regression",
    "section": "",
    "text": "Important\n\n\n\nThis page is currently under construction, and being slowly updated with better explanations and formatting."
  },
  {
    "objectID": "pages/faces.html",
    "href": "pages/faces.html",
    "title": "Eigenfaces",
    "section": "",
    "text": "# Spatial data rasterization package\nif (!require(\"terra\")) {\n    remotes::install_github(\"rspatial/terra\", configure.args = \"--with-proj-lib=$(brew --prefix)/lib/\")\n}\n\nLoading required package: terra\n\n\nterra 1.7.24\n\nlibrary(\"terra\")\n\n# Raw \"faces.dat\" data as dataframe. Each row is one face, and the 4096 columns\n# each represent a pixel value in the face, in a 64 x 64 image.\nfacedata_raw &lt;- read.delim(file = \"../data/faces.dat\", header = FALSE, sep = \"\")\n\n# Creates an image of a face given it's 4096 pixel values\ncreate_face &lt;- function(values) {\n    # New SpatRaster object with correct face parameters\n    face_rast &lt;- rast(nrows = 64, ncols = 64)\n    # Set extent\n    ext(face_rast) &lt;- c(0, 64, 0, 64)\n    # Set pixel values\n    values(face_rast) &lt;- values\n    # Return face rotated to upright orientation\n    return(trans(face_rast))\n}\n\n# Displays a face in grayscale colour\nshow_face_grayscale &lt;- function(rast) {\n    par(xaxt = \"n\", yaxt = \"n\")\n    cols &lt;- gray(seq(0, 1, length = 256))\n    image(rast, col = cols)\n}\n\n# Displays a face using a divergent colour scale\nshow_face_divergent &lt;- function(rast) {\n    par(xaxt = \"n\", yaxt = \"n\")\n    plot(rast)\n}\n\n\n\n\n\n\n# Vector of pixel values of 100th face\nvector_100 &lt;- t(facedata_raw[100, ])\n\n# Rasterize and show face\nface_raw_100 &lt;- create_face(vector_100)\nshow_face_grayscale(face_raw_100)\ntitle(\"100th Face\")\n\n\n\n\n\n\n\n\n\n# The mean vector of face pixel values\nvector_mean &lt;- as.matrix(colMeans(facedata_raw))\n# Rasterize and show mean face\nface_mean &lt;- create_face(vector_mean)\nshow_face_grayscale(face_mean)\ntitle(\"Mean Face\")\n\n\n\n# Face data with the mean face vector removed\nfaces_removed &lt;- facedata_raw\n# Remove mean vector from each face\nfor (i in seq_len(nrow(faces_removed))) {\n    face &lt;- faces_removed[i, ]\n    face_new &lt;- face - t(vector_mean)\n    faces_removed[i, ] &lt;- face_new\n}\n\n# Vector of pixel values of 100th face, with the mean removed\nvector_removed_100 &lt;- t(faces_removed[100, ])\n\n# Rasterize and show face\nface_removed_100 &lt;- create_face(vector_removed_100)\npar(mfrow = c(1, 2))\nshow_face_grayscale(face_removed_100)\nshow_face_divergent(face_removed_100)\n\n\n\n\nThe 100th face with the mean face values subtracted, rasterized. On the left is a grayscale image; on the right is colourized using a divergent colour scale to better visualize negative values. \n\n\n\n\n# Principal component analysis of faces with mean removed\npca &lt;- prcomp(faces_removed, center = FALSE)\n\n# Principal components from PCA\npcs &lt;- pca$x\n# Eigenvectors from PCA\nevecs &lt;- pca$rotation\n# Eigenvalues from PCA\nevals &lt;- pca$sdev^2\n# Proportion of variance from PCA\npv &lt;- evals / sum(evals)\n# Cumulative proportion of variance from PCA\npv_cum &lt;- cumsum(evals) / sum(evals)\n\n# Plot scree plots of Eigenvalues\nplot(\n    pv * 100,\n    type = \"b\",\n    ylim = c(0, 25),\n    xlab = \"Eigenvalues\",\n    ylab = \"Proportion of variance (percentage)\",\n    main = \"Proportion explained by Eigenvalues\"\n)\n\n\n\nplot(\n    pv_cum * 100,\n    type = \"b\",\n    ylim = c(0, 100),\n    xlab = \"Eigenvalues\",\n    ylab = \"Cumulative Proportion of variance (percentage)\",\n    main = \"Cumulative Proportion of variance explained by Eigenvalues\"\n)\n\n\n\n\nEigenvalues plotted against proportion of variance explained. The first plot shows individual Eigenvalue variance percentages sorted in descending order, whilst the second plot shows the cumulative proportion for each descending Eigenvalue. \n\n\n\n\n# Last (400th) Eigenvalue\neval_400 &lt;- evals[400]\neval_400\n\n[1] 1.797793e-25\n\n\nThe 400th Eigenvalue shown above is equal to zero (shown as a very small number due to the limitations of “numerics” in R, essentially equivalent to zero). This can be expected because Eigenvalues represent the variance in the values based on each principal component (PC). Since they are sorted in descending order, we can see that the first few PCs, especially the first one, contribute a very large amount to the variance of the face data. Conversely, the Eigenvalues drop off rapidly until our final Eigenvalue of zero. An Eigenvalue of zero means that the PC associated with it does not contribute to the variance of the data much at all, and the first 399 values have already captured the variability. \n\n\n\n\n# Threshold to keep Eigenvalues explaining at least this amount of variance\nthreshold &lt;- 0.95\n\n# Show cumulative variance with threshold marked\npar(mfrow = c(1, 1))\nplot(\n    pv_cum * 100,\n    type = \"b\",\n    ylim = c(0, 100),\n    xlab = \"Eigenvalues\",\n    ylab = \"Cumulative Proportion of variance (percentage)\",\n    main = \"A: Threshold of 0.95 visualized\"\n)\nabline(h = 95, lty = \"dashed\", col = \"red\")\n\n\n\n# Number of Eigenvalues/PCs to keep based on threshold\nkeep &lt;- min(which(pv_cum &gt; threshold))\nkeep\n\n[1] 123\n\n# Show cumulative variance with threshold marked, with cropped x-axis\nplot(\n    pv_cum * 100,\n    type = \"b\",\n    ylim = c(0, 100),\n    xlim = c(0, 140),\n    xlab = \"Eigenvalues (cropped)\",\n    ylab = \"Cumulative Proportion of variance (percentage)\",\n    main = \"B: Threshold of 0.95 visualized for first 140 Eigenvalues\"\n)\nabline(h = 95, lty = \"dashed\", col = \"red\")\n\n\n\n\nTo account for most of the variance, a threshold can be set to determine the number of prinicpal components (PCs) that account for this proportion of variance. For example, with a threshold of 50%, or 0.50, you would determine at which number of PCs the cumulative proportion of variance explained reaches 50% of the total.\nI set the variance at 0.95, and determined the number of Eigenvalues to keep based on it. The first plot (A) shows the cumulative proportion of variance associated with each Eigenvalue, and we can see a red dashed line visualizing the cumulative variance proportion of 0.95. After determining the intersect of the scree plot and the threshold, I zoomed into the plot to better see individual points on the cumulative scree plot. 95% of the variance is explained by the first 123 PCs, which already removes 277 less necessary PCs. This way, we can still account for most of the variance at 95%, whilst cutting the number of PCs down to under a third of the size. \n\n\n\n\n# Defines how many top Eigenvectors to take (i.e. the top n values)\nn_top &lt;- 5\n\n# The top 5 Eigenvectors (PCs)\nevecs_topn &lt;- evecs[, 1:n_top]\n\n# Iterate and show top n Eigenvectors\nfor (i in seq_len(n_top)) {\n    # Current Eigenvector\n    evec &lt;- evecs_topn[, i]\n    # Rasterize and show the Eigenvector visualization\n    par(mfrow = c(1, 2))\n    face_evec &lt;- create_face(evec)\n    show_face_grayscale(face_evec)\n    title(paste0(\"Eigenvector \", i))\n    show_face_divergent(face_evec)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the Eigenvector values are relative and not in standard 8-bit colour format, they are visualized above as both grayscale and colourized using diverging colour scales. \n\n\n\n\n# List of number of principal components to use in reconstructions\nn_pcs &lt;- c(10, 100, 200, 399)\n\n# nth face to be reconstructed\nn_face &lt;- 100\n\n# Rasterize mean face, raw face, and face with mean removed for reference\npar(mfrow = c(1, 3))\nshow_face_grayscale(face_mean)\ntitle(\"Mean Face\")\nvector_raw_n &lt;- t(facedata_raw[n_face, ])\nface_raw_n &lt;- create_face(vector_raw_n)\nshow_face_grayscale(face_raw_n)\ntitle(\"Raw Face Image\")\n# Face vector of 4096 pixel values with the mean removed for reconstruction\nvector_removed_n &lt;- t(faces_removed[n_face, ])\nface_removed_n &lt;- create_face(vector_removed_n)\nshow_face_grayscale(face_removed_n)\ntitle(\"Mean Removed\")\n\n\n\n# Iterate and reconstruct specifed face with specifed PCs\npar(mfrow = c(1, length(n_pcs) / 2))\nfor (n in seq_len(length(n_pcs))) {\n    # The top n selected principal components\n    evecs_selected &lt;- as.matrix(evecs[, 1:n_pcs[n]])\n    # Projection of face with mean subtracted onto selected Eigenvectors\n    proj &lt;- t(vector_removed_n) %*% evecs_selected\n    # Reconstructed vector from projection and Eigenvectors with mean added back\n    vector_reconstr &lt;- t(proj %*% t(evecs_selected)) + vector_mean\n\n    # Rasterize and show reconstructed face\n    face_reconstr &lt;- create_face(vector_reconstr)\n    show_face_grayscale(face_reconstr)\n    title(paste0(n_pcs[n], \" PCs\"))\n}\n\n\n\n\n\n\n\nWe can see clearly that the more principal components used in the reconstruction, the better the reconstruction is, and the closer it is to the original raw face. With only the top 10 principal components, the face reconstruction looks very similar to the mean. With 399 principal components, the reconstruction is virtually identical to the raw."
  },
  {
    "objectID": "pages/filters.html",
    "href": "pages/filters.html",
    "title": "Edge and Corner Detection",
    "section": "",
    "text": "# load libraries\nsuppressPackageStartupMessages({\n    library(imager)\n    library(ggplot2)\n    library(patchwork)\n})\n\n# load image from file into R as a cimg object\nimg &lt;- load.image(\"../data/image1.png\")\nclass(img)\n\n[1] \"cimg\"         \"imager_array\" \"numeric\"     \n\n\nUsing the imager library, all cimg objects are represented as 4-dimensional arrays. The dimensions represent: width, height, depth, and colour. In our case, our input image is already in grayscale, and we are only concerned with each pixel value, thus the dimensions for depth and colour are largely ignored. Nonetheless, we have to initialize our image as well as all subsequent filters and images in our code to contain all four dimensions so that they will be able to convolve.\n\n# see dimensions of image\ndim(img)\n\n[1] 276 182   1   1\n\n# view summary of image data\nimg\n\nImage. Width: 276 pix Height: 182 pix Depth: 1 Colour channels: 1 \n\n\n\n# helper function applies convolution to an image given a filter\napply_convolution &lt;- function(image, filter) {\n    # Get dimensions of the image and the filter\n    img_dim &lt;- dim(image)\n    filt_dim &lt;- dim(filter)\n\n    # Define padding size\n    padding &lt;- floor(filt_dim[1] / 2)\n\n    # initialize padded image as empty array\n    img_padded &lt;- array(0, dim = c(\n        img_dim[1] + 2 * padding, img_dim[2] + 2 * padding, img_dim[3], img_dim[4]\n    ))\n\n    # populate padded image array with image pixels\n    img_padded[\n        (1 + padding):(img_dim[1] + padding), # width\n        (1 + padding):(img_dim[2] + padding), , # height\n    ] &lt;- image\n\n    # initialize convolved image as empty array\n    img_convolved &lt;- array(0, dim = img_dim)\n\n    # iterate each pixel in image and convolve with filter\n    for (i in 1:img_dim[1]) {\n        for (j in 1:img_dim[2]) {\n            pixel_convolved &lt;- 0\n            for (k in 1:filt_dim[1]) {\n                for (l in 1:filt_dim[2]) {\n                    pixel_convolved &lt;- pixel_convolved +\n                        filter[k, l, , ] *\n                            img_padded[i + k - 1, j + l - 1, , ]\n                }\n            }\n            img_convolved[i, j, , ] &lt;- pixel_convolved\n        }\n    }\n\n    return(as.cimg(img_convolved))\n}\n\n\n# helper function displays images, given an image and title\nplot_img &lt;- function(img, title = \"\") {\n    # convert image to dataframe\n    img_df &lt;- as.data.frame(img)\n\n    # generate plot of dataframe\n    plot &lt;- ggplot(img_df, aes(x, y)) + # nolint\n        # define rasterization\n        geom_raster(aes(fill = value), show.legend = FALSE) + # nolint\n        # define color scale (grayscale)\n        scale_fill_gradient(low = \"black\", high = \"white\") +\n        # fix pixel ratio / aspect ratio\n        coord_fixed() +\n        # reverse y scale (image orientation)\n        scale_y_reverse() +\n        # set title\n        ggtitle(title) +\n        # remove all graphical details from plot besides image\n        theme_void()\n\n    return(plot)\n}\n\n# display the original image\nplot_img(img, \"Original Image\")\n\n\n\n\n\n\n\n\n\n\n\n\nImplement convolution process of smoothing Image 1 with a 5 x 5 Gaussian Filter with σ = 1 and 2, plot the corresponding output images.\n\n# function applies a kxk Gaussian filter to an image with a sigma value\napply_gaussian &lt;- function(img, ksize, sigma) {\n    # initialize x and y vectors around 0, e.g. [-2, -1, 0, 1, 2]\n    center &lt;- floor(ksize / 2)\n    x &lt;- y &lt;- seq(-center, center, 1)\n\n    # Gaussian function\n    gauss &lt;- outer(x, y, function(x, y) {\n        exp(-(x^2 + y^2) / (2 * sigma^2))\n    })\n\n    # initialize kernel as empty array\n    kernel &lt;- array(0, dim = c(ksize, ksize, 1, 1))\n    # populate kernel with Gaussian filter values\n    kernel[, , 1, 1] &lt;- gauss\n\n    # display kernel values\n    print(\"Resulting kernel:\")\n    print(kernel[, , 1, 1])\n\n    # convolve image with kernel\n    img_flt &lt;- apply_convolution(img, kernel)\n    return(img_flt)\n}\n\n# apply Gaussian filter to image with sigma = 1\nimg_gauss_1 &lt;- apply_gaussian(img, ksize = 5, sigma = 1)\n\n[1] \"Resulting kernel:\"\n           [,1]      [,2]      [,3]      [,4]       [,5]\n[1,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n[2,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[3,] 0.13533528 0.6065307 1.0000000 0.6065307 0.13533528\n[4,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[5,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n\n# apply Gaussian filter to image with sigma = 2\nimg_gauss_2 &lt;- apply_gaussian(img, ksize = 5, sigma = 2)\n\n[1] \"Resulting kernel:\"\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 0.3678794 0.5352614 0.6065307 0.5352614 0.3678794\n[2,] 0.5352614 0.7788008 0.8824969 0.7788008 0.5352614\n[3,] 0.6065307 0.8824969 1.0000000 0.8824969 0.6065307\n[4,] 0.5352614 0.7788008 0.8824969 0.7788008 0.5352614\n[5,] 0.3678794 0.5352614 0.6065307 0.5352614 0.3678794\n\n# show resulting images side by side\nplot_img(img_gauss_1, \"Sigma = 1\") + plot_img(img_gauss_2, \"Sigma = 2\")\n\n\n\n\n\n\n\n\nWe can see that the blurring effect from the Gaussian filter is stronger with a greater sigma value.\n\n\n\n\nImplement convolution process of convolving a 3 x 3 Sobel filters (Sx and Sy) with the output images you obtained in step a. Plot the outputs of this process.\n\n# define Sobel kernel Sx\nsobel_x &lt;- array(\n    c(\n        -1, 0, 1,\n        -2, 0, 2,\n        -1, 0, 1\n    ),\n    dim = c(3, 3, 1, 1)\n)\n\n# define Sobel kernel Sy\nsobel_y &lt;- array(\n    c(\n        -1, -2, -1,\n        0, 0, 0,\n        1, 2, 1\n    ),\n    dim = c(3, 3, 1, 1)\n)\n\nApplying Sobel filters to Image with Gaussian filter where σ = 1:\n\n# apply Sobel filters to first Gaussian filter output\nimg_g1_sx &lt;- apply_convolution(img_gauss_1, sobel_x) # x filter\nimg_g1_sy &lt;- apply_convolution(img_gauss_1, sobel_y) # y filter\n\n# show resulting images side by side\nplot_img(img_g1_sx, \"Sigma = 1, Sobel x\") +\n    plot_img(img_g1_sy, \"Sigma = 1, Sobel y\")\n\n\n\n\n\n\n\n\nApplying Sobel filters to Image with Gaussian filter where σ = 2:\n\n# apply Sobel filters to second Gaussian filter output\nimg_g2_sx &lt;- apply_convolution(img_gauss_2, sobel_x) # x filter\nimg_g2_sy &lt;- apply_convolution(img_gauss_2, sobel_y) # y filter\n\n# Show resulting images side by side\nplot_img(img_g2_sx, \"Sigma = 2, Sobel x\") +\n    plot_img(img_g2_sy, \"Sigma = 2, Sobel y\")\n\n\n\n\n\n\n\n\nEdge detection is a bit sharper with the lower sigma value, however this comes at the cost of increased noise.\n\n\n\n\nImplement convolution process of convolving Image 1 with \\(\\frac{\\partial G}{\\partial x}\\) and \\(\\frac{\\partial G}{\\partial y}\\) filters (5 x 5 filters and for σ = 1 and 2).\n\n# function applies first partial x-derivative Gaussian filter to an image\napply_d_gaussian_x &lt;- function(img, ksize, sigma) {\n    # initialize x and y vectors around 0, e.g. [-2, -1, 0, 1, 2]\n    center &lt;- floor(ksize / 2)\n    x &lt;- y &lt;- seq(-center, center, 1)\n\n    # first derivative of Gaussian function wrt x\n    dgauss &lt;- outer(x, y, function(x, y) {\n        (-x / sigma^2) * exp(-(x^2 + y^2) / (2 * sigma^2))\n    })\n\n    # initialize kernel as empty array\n    kernel &lt;- array(0, dim = c(ksize, ksize, 1, 1))\n    # populate kernel with x-derivative Gaussian filter values\n    kernel[, , 1, 1] &lt;- dgauss\n\n    # display kernel values\n    print(\"Resulting kernel:\")\n    print(kernel[, , 1, 1])\n\n    # convolve image with kernel window\n    img_flt &lt;- apply_convolution(img, kernel)\n    return(img_flt)\n}\n\n# function applies first partial y-derivative Gaussian filter to an image\napply_d_gaussian_y &lt;- function(img, ksize, sigma) {\n    # initialize x and y vectors around 0, e.g. [-2, -1, 0, 1, 2]\n    center &lt;- floor(ksize / 2)\n    x &lt;- y &lt;- seq(-center, center, 1)\n\n    # first derivative of Gaussian function wrt y\n    dgauss &lt;- outer(x, y, function(x, y) {\n        (-y / sigma^2) * exp(-(x^2 + y^2) / (2 * sigma^2))\n    })\n\n    # initialize kernel as empty array\n    kernel &lt;- array(0, dim = c(ksize, ksize, 1, 1))\n    # populate kernel with y-derivative Gaussian filter values\n    kernel[, , 1, 1] &lt;- dgauss\n\n    # display kernel values\n    print(\"Resulting kernel:\")\n    print(kernel[, , 1, 1])\n\n    # convolve image with kernel window\n    img_flt &lt;- apply_convolution(img, kernel)\n    return(img_flt)\n}\n\n# apply derivative Gaussian filters to Image with sigma = 1\nimg_dg1_x &lt;- apply_d_gaussian_x(img, ksize = 5, sigma = 1) # x-derivative\n\n[1] \"Resulting kernel:\"\n            [,1]       [,2]       [,3]       [,4]        [,5]\n[1,]  0.03663128  0.1641700  0.2706706  0.1641700  0.03663128\n[2,]  0.08208500  0.3678794  0.6065307  0.3678794  0.08208500\n[3,]  0.00000000  0.0000000  0.0000000  0.0000000  0.00000000\n[4,] -0.08208500 -0.3678794 -0.6065307 -0.3678794 -0.08208500\n[5,] -0.03663128 -0.1641700 -0.2706706 -0.1641700 -0.03663128\n\nimg_dg1_y &lt;- apply_d_gaussian_y(img, ksize = 5, sigma = 1) # y-derivative\n\n[1] \"Resulting kernel:\"\n           [,1]      [,2] [,3]       [,4]        [,5]\n[1,] 0.03663128 0.0820850    0 -0.0820850 -0.03663128\n[2,] 0.16417000 0.3678794    0 -0.3678794 -0.16417000\n[3,] 0.27067057 0.6065307    0 -0.6065307 -0.27067057\n[4,] 0.16417000 0.3678794    0 -0.3678794 -0.16417000\n[5,] 0.03663128 0.0820850    0 -0.0820850 -0.03663128\n\n# apply derivative Gaussian filters to Image with sigma = 2\nimg_dg2_x &lt;- apply_d_gaussian_x(img, ksize = 5, sigma = 2) # x-derivative\n\n[1] \"Resulting kernel:\"\n           [,1]       [,2]       [,3]       [,4]       [,5]\n[1,]  0.1839397  0.2676307  0.3032653  0.2676307  0.1839397\n[2,]  0.1338154  0.1947002  0.2206242  0.1947002  0.1338154\n[3,]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000\n[4,] -0.1338154 -0.1947002 -0.2206242 -0.1947002 -0.1338154\n[5,] -0.1839397 -0.2676307 -0.3032653 -0.2676307 -0.1839397\n\nimg_dg2_y &lt;- apply_d_gaussian_y(img, ksize = 5, sigma = 2) # y-derivative\n\n[1] \"Resulting kernel:\"\n          [,1]      [,2] [,3]       [,4]       [,5]\n[1,] 0.1839397 0.1338154    0 -0.1338154 -0.1839397\n[2,] 0.2676307 0.1947002    0 -0.1947002 -0.2676307\n[3,] 0.3032653 0.2206242    0 -0.2206242 -0.3032653\n[4,] 0.2676307 0.1947002    0 -0.1947002 -0.2676307\n[5,] 0.1839397 0.1338154    0 -0.1338154 -0.1839397\n\n# Show resulting images\nplot_img(img_dg1_x, \"Sigma = 1, Derivative Gaussian x\") +\n    plot_img(img_dg1_y, \"Sigma = 1, Derivative Gaussian y\")\n\n\n\n\n\n\n\nplot_img(img_dg2_x, \"Sigma = 2, Derivative Gaussian x\") +\n    plot_img(img_dg2_y, \"Sigma = 2, Derivative Gaussian y\")\n\n\n\n\n\n\n\n\n\n\n\nStep b utilized edge detection using the Sobel filter for x and y, and step c utilized the first derivative Gaussian filter for x and y. We can plot a review of the output images below:\n\n# plot images generated from sigma = 1\nplot_img(img_g1_sx, \"Sigma = 1, Sobel x\") +\n    plot_img(img_g1_sy, \"Sigma = 1, Sobel y\") +\n    plot_img(img_dg1_x, \"Sigma = 1, Derivative Gaussian x\") +\n    plot_img(img_dg1_y, \"Sigma = 1, Derivative Gaussian y\") +\n    plot_layout(ncol = 2)\n\n\n\n\n\n\n\n# plot images generated from sigma = 2\nplot_img(img_g2_sx, \"Sigma = 2, Sobel x\") +\n    plot_img(img_g2_sy, \"Sigma = 2, Sobel y\") +\n    plot_img(img_dg2_x, \"Sigma = 2, Derivative Gaussian x\") +\n    plot_img(img_dg2_y, \"Sigma = 2, Derivative Gaussian y\") +\n    plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\nFrom the rasterizations above, we can see that both filters are effective at their main goal of edge detection. On it’s own, the Sobel filters is known to be sensitive to noise, and so we first filtered the image using Gaussian filters before applying the Sobel filters. The first derivative Gaussian filter does not have as much sensitivity. One noticeable difference is the opposite visual effects the filters have; the edges that appear to be protruding in the Sobel filter, appear to be sunken in the derivative Gaussian filter. This likely results from how the two filters emphasize the orientation of edges, with opposite negative and positive values in the resulting image. Finally, another difference observed between the two filters is that the edges are lighter and thicker with the Sobel filters, whilst the edges are darker and sharper with the derivative Gaussian filter (albeit to a slim extent). This may be attributed to the extra step of pre-filtering the image with a Gaussian filter before applying the Sobel filter, which adds an extra level of blur. On the other hand, the derivative Gaussian filter both removes noise and detects edges in one filter, which removes an extra step needed, also resulting in less of a blurring effect."
  },
  {
    "objectID": "pages/filters.html#setup-define-parameters-used-in-harris-corner-detection",
    "href": "pages/filters.html#setup-define-parameters-used-in-harris-corner-detection",
    "title": "Edge and Corner Detection",
    "section": "Setup: define parameters used in Harris Corner Detection",
    "text": "Setup: define parameters used in Harris Corner Detection\n\n# window size for Gaussian filter (w x w)\nw &lt;- 5\n# sigma for Gaussian filter\nsigma &lt;- 1\n\n# empirically determined constant used in corner response calculation\nk &lt;- 0.04"
  },
  {
    "objectID": "pages/filters.html#step-1.-spatial-derivative-calculation",
    "href": "pages/filters.html#step-1.-spatial-derivative-calculation",
    "title": "Edge and Corner Detection",
    "section": "Step 1. Spatial derivative calculation",
    "text": "Step 1. Spatial derivative calculation\n\n# apply Sobel filters to Image\nI_x &lt;- apply_convolution(img, sobel_x)\nI_y &lt;- apply_convolution(img, sobel_y)\n\n# compute products of derivatives\nI_xx &lt;- I_x^2\nI_yy &lt;- I_y^2\nI_xy &lt;- I_x * I_y"
  },
  {
    "objectID": "pages/filters.html#step-2.-structure-tensor-setup-m",
    "href": "pages/filters.html#step-2.-structure-tensor-setup-m",
    "title": "Edge and Corner Detection",
    "section": "Step 2. Structure tensor setup (M)",
    "text": "Step 2. Structure tensor setup (M)\n\n# compute sums of products of derivatives\nS_xx &lt;- apply_gaussian(I_xx, w, sigma)\n\n[1] \"Resulting kernel:\"\n           [,1]      [,2]      [,3]      [,4]       [,5]\n[1,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n[2,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[3,] 0.13533528 0.6065307 1.0000000 0.6065307 0.13533528\n[4,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[5,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n\nS_yy &lt;- apply_gaussian(I_yy, w, sigma)\n\n[1] \"Resulting kernel:\"\n           [,1]      [,2]      [,3]      [,4]       [,5]\n[1,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n[2,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[3,] 0.13533528 0.6065307 1.0000000 0.6065307 0.13533528\n[4,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[5,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n\nS_xy &lt;- apply_gaussian(I_xy, w, sigma)\n\n[1] \"Resulting kernel:\"\n           [,1]      [,2]      [,3]      [,4]       [,5]\n[1,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n[2,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[3,] 0.13533528 0.6065307 1.0000000 0.6065307 0.13533528\n[4,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[5,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n\n# note: see Edge Detection (a) for implementation of apply_gaussian()\n\n# initialize empty matrix M to store sums\nM &lt;- array(0, dim = c(nrow(img), ncol(img), 2, 2))\n\n# populate matrix M with sums\nfor (i in seq_len(nrow(img))) {\n    for (j in seq_len(ncol(img))) {\n        # insert 2x2 matrix for each pixel\n        M[i, j, , ] &lt;- matrix(\n            c(\n                S_xx[i, j, , ], S_xy[i, j, , ],\n                S_xy[i, j, , ], S_yy[i, j, , ]\n            ),\n            nrow = 2\n        )\n    }\n}"
  },
  {
    "objectID": "pages/filters.html#step-3.-corner-response-calculation",
    "href": "pages/filters.html#step-3.-corner-response-calculation",
    "title": "Edge and Corner Detection",
    "section": "Step 3. Corner response calculation",
    "text": "Step 3. Corner response calculation\n\n# initialize empty matrix R to store measure of corner response\nR &lt;- array(0, dim = c(nrow(img), ncol(img), 1, 1))\n\n# populate matrix R with corner responses per pixel\nfor (i in seq_len(nrow(img))) {\n    for (j in seq_len(ncol(img))) {\n        # extract matrix M for current pixel\n        M_ij &lt;- matrix(M[i, j, , ], nrow = 2, ncol = 2)\n\n        # compute eigenvalues\n        eigenM &lt;- eigen(M_ij)\n        lambda1 &lt;- eigenM$values[1] # first eigenvalue of current M\n        lambda2 &lt;- eigenM$values[2] # second eigenvalue of current M\n\n        # compute determinant and trace using eigenvalues\n        detM &lt;- lambda1 * lambda2\n        traceM &lt;- lambda1 + lambda2\n\n        # compute corner response and add to matrix R\n        R[i, j, , ] &lt;- detM - k * traceM^2\n    }\n}\n\n# plot results\ncorner_response_map &lt;- as.cimg(R)\nplot_img(corner_response_map, \"Corner response map\")\n\n\n\n\n\n\n\n\nIn our corner response map, we can see that white areas correspond to corners, and black areas correspond to edges."
  },
  {
    "objectID": "pages/filters.html#step-4.-non-maximum-suppression",
    "href": "pages/filters.html#step-4.-non-maximum-suppression",
    "title": "Edge and Corner Detection",
    "section": "Step 4. Non-maximum suppression",
    "text": "Step 4. Non-maximum suppression\n\n# view summary of our corner responses\nsummary(R)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-49.03308  -0.01485   0.00000  -0.02343   0.00059 202.64260 \n\n# view histogram of our corner responses\nhist(R, breaks = 100, xlab = \"Corner response value\")\n\n\n\n\n\n\n\n\nFrom our summary, we can see the the values are distributed around a center of approximately 0. Since greater positive values correspond to a higher corner detection response, I can set a threshold of 0 to capture any pixels with a positive value. In the histogram, we can see that the vast majority of pixels have a value of around 0, while very few have values over 50, or even 25. We can increase the threshold up depending on how sharply we want to define where corners are detected.\n\n# corner R value threshold\nthreshold &lt;- 0\n\n# keep only values meeting threshold, setting otherwise to 0 (black)\nR_thr &lt;- R\nR_thr[R_thr &lt; threshold] &lt;- 0\n\n# plot results\nnon_max_suppression &lt;- as.cimg(R_thr)\nplot_img(non_max_suppression, \"Non-max suppression, threshold = 0\")\n\n\n\n\n\n\n\n\nIn this image, only corners are displayed in white, while all other non-corner pixels are left out of the image (i.e. are black). We can see that the Harris corner detection highlights the corners of each window of the building, as well as corners of the roof and building itself.\nFor comparison’s sake, we can increase the threshold to only include pixels with a much higher positive corner response:\n\n# increased corner R value threshold\nthreshold &lt;- 80\n\n# plot results\nR_thr &lt;- R\nR_thr[R_thr &lt; threshold] &lt;- 0\nnon_max_suppression &lt;- as.cimg(R_thr)\nplot_img(non_max_suppression, \"Non-max suppression, threshold = 80\")\n\n\n\n\n\n\n\n\nHere our results are much sharper and excludes possible corners brought about by noise."
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\nHi my name is Michael and this page is not done."
  },
  {
    "objectID": "pages/linreg.html#linear-regression-formula",
    "href": "pages/linreg.html#linear-regression-formula",
    "title": "Simple Regression",
    "section": "Linear Regression Formula",
    "text": "Linear Regression Formula\nWe’ll work with the training data only for now, as once we have a weights vector and a completed model, we can then use the testing data to evaluate the model. To relate our data to the linear regression model, we first have to add the column vector of 1’s to the features to account for the bias term.\n\n# Add column of 1's to training features\n1x_tr_1 &lt;- cbind(\n2    x_tr,\n3    b = rep(1, length(x_tr))\n)\n\n\n1\n\nCreate a new variable, x_tr_1, that is the result of the cbind function. This function takes vectors (or dataframes) and combines them by their columns, hence the name cbind for column bind. We will want to combine our original features with a column of 1’s of the same length.\n\n2\n\nSpecify our first vector to combine, x_tr. This is the original training features vector.\n\n3\n\nThe second vector to combine will be a column containing just ones of the same length as our orignal vector. We can create this column of 1’s on the fly (instead of manually) by using the rep function. The rep function takes a value and repeats it a specified number of times. In this case, we want to repeat the value 1, length(x_tr) times. The length function returns the length of a vector, in this case the number of elements in x_tr.\n\n\n\n\nNow x_tr_1 looks like this:\n\nhead(x_tr_1)\n\n       x_tr b\n1 3.1381904 1\n2 3.4421490 1\n3 1.3083408 1\n4 0.9954419 1\n5 3.2414549 1\n6 2.8537392 1\n\n\n\n\n\n\n\n\nLike with our _tr suffix, I use the _1 suffix to represent variables for linear regression. Later on I’ll be using other regression models such as _2 for second-order polynomial regression, _3 for third-order, etc.\n\n\n\nNow that we have our training features with the bias term included, we can use the training labels to solve for the weights vector by rearranging the linear regression formula. Using matrix algebra, we derive the formula for solving for the weights vector:\n\\[w = (X^\\top X)^{-1} X^\\top y\\]\nwhere \\(X\\) is the matrix of training features, \\(y\\) is the vector of training labels, and \\(w\\) is the vector of weights. The \\(X^\\top\\) notation is the transpose of \\(X\\), and the \\(X^{-1}\\) notation is the inverse of \\(X\\). The inverse of a matrix is a matrix that when multiplied by the original matrix, results in the identity matrix. The identity matrix is a square matrix with 1’s on the diagonal and 0’s everywhere else. The identity matrix is the multiplicative identity, meaning that when multiplied by any matrix, the result is the original matrix. The inverse of a matrix is the multiplicative inverse, meaning that when multiplied by the original matrix, the result is the identity matrix.\nMatrix operations are available in base R, including %*% for matrix multiplication, t for transposing a matrix, and solve for solving a matrix equation. When solve is only provided one parameter, it solves for the inverse of the input matrix. We can use these functions to solve for the weights vector.\n\n# Convert training dataframes to matrices\n1y_tr_mat &lt;- as.matrix(y_tr)\nx_tr_1 &lt;- as.matrix(x_tr_1)\n\n# Solve for weight vector using training data\n2w &lt;- solve(t(x_tr_1) %*% x_tr_1) %*% (t(x_tr_1) %*% y_tr_mat)\n\n\n1\n\nTo perform matrix operations on our dataframes, we need to first convert them to matrices. We can do this using the as.matrix function. I rename y_tr to y_tr_mat to store its matrix variable separately, since it will be reused later on in the code.\n\n2\n\nSolve for w using the formula above, with x_tr_1 as X and y_tr_mat as y. We use the %*% operator for matrix multiplication, the solve function to solve for the inverse of a matrix, and the t function to transpose a matrix. The brackets are there to visually support the order of operations.\n\n\n\n\nNow that we have our weights vector \\(w\\), the full linear regression formula is complete and we can plot the resulting function on top of our training data.\n\n# Plot linear regression on training set\nplt_tr_1 &lt;- ggplot(\n    data = df_tr,\n    mapping = aes(x = x_tr, y = y_tr)\n) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        w[1] * x + w[2]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\n\nplt_tr_1"
  },
  {
    "objectID": "pages/linreg.html#average-error",
    "href": "pages/linreg.html#average-error",
    "title": "Simple Regression",
    "section": "Average Error",
    "text": "Average Error\n\n# compute average training error\neq_tr_1 &lt;- t(as.data.frame(t(w) %*% t(x_tr_1)))\nerrors_tr_1 &lt;- (eq_tr_1 - y_tr_mat)**2\nerr_avg_tr_1 &lt;- colSums(errors_tr_1) / length(errors_tr_1)\nerr_avg_tr_1\n\n    y_tr \n2.143112 \n\n\nThe average error of the training set using the linear regression model is 2.287073.\nPlot both the regression line and the test data on the same graph. Also report the average error on the test set using Eq. (1).\n\n# convert training outputs to matrix\ny_te_mat &lt;- as.matrix(y_te)\n\n# add column of 1s to testing features\nx_te_1 &lt;- cbind(x_te, b = rep(1, length(x_te)))\nx_te_1 &lt;- as.matrix(x_te_1)\n\n# plot linear regression on test set\nplt_te_1 &lt;- ggplot(\n    data = df_te,\n    mapping = aes(x = x_te, y = y_te)\n) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        w[1] * x + w[2]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_1\n\n\n\n# compute average test error using same weight vector from training data\neq_te_1 &lt;- t(as.data.frame(t(w) %*% t(x_te_1)))\nerrors_te_1 &lt;- (eq_te_1 - y_te_mat)**2\nerr_avg_te_1 &lt;- colSums(errors_te_1) / length(errors_te_1)\nerr_avg_te_1\n\n    y_tr \n9.973294 \n\n# store average errors:\nerrs &lt;- data.frame(\n    \"training\" = err_avg_tr_1,\n    \"testing\" = err_avg_te_1,\n    row.names = \"order1\"\n)\n\nThe average error of the testing set using the linear regression model is 9.427358."
  },
  {
    "objectID": "pages/faces.html#initial-setup-and-utility-functions",
    "href": "pages/faces.html#initial-setup-and-utility-functions",
    "title": "Eigenfaces",
    "section": "",
    "text": "# Spatial data rasterization package\nif (!require(\"terra\")) {\n    remotes::install_github(\"rspatial/terra\", configure.args = \"--with-proj-lib=$(brew --prefix)/lib/\")\n}\n\nLoading required package: terra\n\n\nterra 1.7.24\n\nlibrary(\"terra\")\n\n# Raw \"faces.dat\" data as dataframe. Each row is one face, and the 4096 columns\n# each represent a pixel value in the face, in a 64 x 64 image.\nfacedata_raw &lt;- read.delim(file = \"../data/faces.dat\", header = FALSE, sep = \"\")\n\n# Creates an image of a face given it's 4096 pixel values\ncreate_face &lt;- function(values) {\n    # New SpatRaster object with correct face parameters\n    face_rast &lt;- rast(nrows = 64, ncols = 64)\n    # Set extent\n    ext(face_rast) &lt;- c(0, 64, 0, 64)\n    # Set pixel values\n    values(face_rast) &lt;- values\n    # Return face rotated to upright orientation\n    return(trans(face_rast))\n}\n\n# Displays a face in grayscale colour\nshow_face_grayscale &lt;- function(rast) {\n    par(xaxt = \"n\", yaxt = \"n\")\n    cols &lt;- gray(seq(0, 1, length = 256))\n    image(rast, col = cols)\n}\n\n# Displays a face using a divergent colour scale\nshow_face_divergent &lt;- function(rast) {\n    par(xaxt = \"n\", yaxt = \"n\")\n    plot(rast)\n}"
  },
  {
    "objectID": "pages/faces.html#a",
    "href": "pages/faces.html#a",
    "title": "Eigenfaces",
    "section": "",
    "text": "# Vector of pixel values of 100th face\nvector_100 &lt;- t(facedata_raw[100, ])\n\n# Rasterize and show face\nface_raw_100 &lt;- create_face(vector_100)\nshow_face_grayscale(face_raw_100)\ntitle(\"100th Face\")"
  },
  {
    "objectID": "pages/faces.html#b",
    "href": "pages/faces.html#b",
    "title": "Eigenfaces",
    "section": "",
    "text": "# The mean vector of face pixel values\nvector_mean &lt;- as.matrix(colMeans(facedata_raw))\n# Rasterize and show mean face\nface_mean &lt;- create_face(vector_mean)\nshow_face_grayscale(face_mean)\ntitle(\"Mean Face\")\n\n\n\n# Face data with the mean face vector removed\nfaces_removed &lt;- facedata_raw\n# Remove mean vector from each face\nfor (i in seq_len(nrow(faces_removed))) {\n    face &lt;- faces_removed[i, ]\n    face_new &lt;- face - t(vector_mean)\n    faces_removed[i, ] &lt;- face_new\n}\n\n# Vector of pixel values of 100th face, with the mean removed\nvector_removed_100 &lt;- t(faces_removed[100, ])\n\n# Rasterize and show face\nface_removed_100 &lt;- create_face(vector_removed_100)\npar(mfrow = c(1, 2))\nshow_face_grayscale(face_removed_100)\nshow_face_divergent(face_removed_100)\n\n\n\n\nThe 100th face with the mean face values subtracted, rasterized. On the left is a grayscale image; on the right is colourized using a divergent colour scale to better visualize negative values."
  },
  {
    "objectID": "pages/faces.html#c",
    "href": "pages/faces.html#c",
    "title": "Eigenfaces",
    "section": "",
    "text": "# Principal component analysis of faces with mean removed\npca &lt;- prcomp(faces_removed, center = FALSE)\n\n# Principal components from PCA\npcs &lt;- pca$x\n# Eigenvectors from PCA\nevecs &lt;- pca$rotation\n# Eigenvalues from PCA\nevals &lt;- pca$sdev^2\n# Proportion of variance from PCA\npv &lt;- evals / sum(evals)\n# Cumulative proportion of variance from PCA\npv_cum &lt;- cumsum(evals) / sum(evals)\n\n# Plot scree plots of Eigenvalues\nplot(\n    pv * 100,\n    type = \"b\",\n    ylim = c(0, 25),\n    xlab = \"Eigenvalues\",\n    ylab = \"Proportion of variance (percentage)\",\n    main = \"Proportion explained by Eigenvalues\"\n)\n\n\n\nplot(\n    pv_cum * 100,\n    type = \"b\",\n    ylim = c(0, 100),\n    xlab = \"Eigenvalues\",\n    ylab = \"Cumulative Proportion of variance (percentage)\",\n    main = \"Cumulative Proportion of variance explained by Eigenvalues\"\n)\n\n\n\n\nEigenvalues plotted against proportion of variance explained. The first plot shows individual Eigenvalue variance percentages sorted in descending order, whilst the second plot shows the cumulative proportion for each descending Eigenvalue."
  },
  {
    "objectID": "pages/faces.html#d",
    "href": "pages/faces.html#d",
    "title": "Eigenfaces",
    "section": "",
    "text": "# Last (400th) Eigenvalue\neval_400 &lt;- evals[400]\neval_400\n\n[1] 1.797793e-25\n\n\nThe 400th Eigenvalue shown above is equal to zero (shown as a very small number due to the limitations of “numerics” in R, essentially equivalent to zero). This can be expected because Eigenvalues represent the variance in the values based on each principal component (PC). Since they are sorted in descending order, we can see that the first few PCs, especially the first one, contribute a very large amount to the variance of the face data. Conversely, the Eigenvalues drop off rapidly until our final Eigenvalue of zero. An Eigenvalue of zero means that the PC associated with it does not contribute to the variance of the data much at all, and the first 399 values have already captured the variability."
  },
  {
    "objectID": "pages/faces.html#e",
    "href": "pages/faces.html#e",
    "title": "Eigenfaces",
    "section": "",
    "text": "# Threshold to keep Eigenvalues explaining at least this amount of variance\nthreshold &lt;- 0.95\n\n# Show cumulative variance with threshold marked\npar(mfrow = c(1, 1))\nplot(\n    pv_cum * 100,\n    type = \"b\",\n    ylim = c(0, 100),\n    xlab = \"Eigenvalues\",\n    ylab = \"Cumulative Proportion of variance (percentage)\",\n    main = \"A: Threshold of 0.95 visualized\"\n)\nabline(h = 95, lty = \"dashed\", col = \"red\")\n\n\n\n# Number of Eigenvalues/PCs to keep based on threshold\nkeep &lt;- min(which(pv_cum &gt; threshold))\nkeep\n\n[1] 123\n\n# Show cumulative variance with threshold marked, with cropped x-axis\nplot(\n    pv_cum * 100,\n    type = \"b\",\n    ylim = c(0, 100),\n    xlim = c(0, 140),\n    xlab = \"Eigenvalues (cropped)\",\n    ylab = \"Cumulative Proportion of variance (percentage)\",\n    main = \"B: Threshold of 0.95 visualized for first 140 Eigenvalues\"\n)\nabline(h = 95, lty = \"dashed\", col = \"red\")\n\n\n\n\nTo account for most of the variance, a threshold can be set to determine the number of prinicpal components (PCs) that account for this proportion of variance. For example, with a threshold of 50%, or 0.50, you would determine at which number of PCs the cumulative proportion of variance explained reaches 50% of the total.\nI set the variance at 0.95, and determined the number of Eigenvalues to keep based on it. The first plot (A) shows the cumulative proportion of variance associated with each Eigenvalue, and we can see a red dashed line visualizing the cumulative variance proportion of 0.95. After determining the intersect of the scree plot and the threshold, I zoomed into the plot to better see individual points on the cumulative scree plot. 95% of the variance is explained by the first 123 PCs, which already removes 277 less necessary PCs. This way, we can still account for most of the variance at 95%, whilst cutting the number of PCs down to under a third of the size."
  },
  {
    "objectID": "pages/faces.html#f",
    "href": "pages/faces.html#f",
    "title": "Eigenfaces",
    "section": "",
    "text": "# Defines how many top Eigenvectors to take (i.e. the top n values)\nn_top &lt;- 5\n\n# The top 5 Eigenvectors (PCs)\nevecs_topn &lt;- evecs[, 1:n_top]\n\n# Iterate and show top n Eigenvectors\nfor (i in seq_len(n_top)) {\n    # Current Eigenvector\n    evec &lt;- evecs_topn[, i]\n    # Rasterize and show the Eigenvector visualization\n    par(mfrow = c(1, 2))\n    face_evec &lt;- create_face(evec)\n    show_face_grayscale(face_evec)\n    title(paste0(\"Eigenvector \", i))\n    show_face_divergent(face_evec)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the Eigenvector values are relative and not in standard 8-bit colour format, they are visualized above as both grayscale and colourized using diverging colour scales."
  },
  {
    "objectID": "pages/faces.html#g",
    "href": "pages/faces.html#g",
    "title": "Eigenfaces",
    "section": "",
    "text": "# List of number of principal components to use in reconstructions\nn_pcs &lt;- c(10, 100, 200, 399)\n\n# nth face to be reconstructed\nn_face &lt;- 100\n\n# Rasterize mean face, raw face, and face with mean removed for reference\npar(mfrow = c(1, 3))\nshow_face_grayscale(face_mean)\ntitle(\"Mean Face\")\nvector_raw_n &lt;- t(facedata_raw[n_face, ])\nface_raw_n &lt;- create_face(vector_raw_n)\nshow_face_grayscale(face_raw_n)\ntitle(\"Raw Face Image\")\n# Face vector of 4096 pixel values with the mean removed for reconstruction\nvector_removed_n &lt;- t(faces_removed[n_face, ])\nface_removed_n &lt;- create_face(vector_removed_n)\nshow_face_grayscale(face_removed_n)\ntitle(\"Mean Removed\")\n\n\n\n# Iterate and reconstruct specifed face with specifed PCs\npar(mfrow = c(1, length(n_pcs) / 2))\nfor (n in seq_len(length(n_pcs))) {\n    # The top n selected principal components\n    evecs_selected &lt;- as.matrix(evecs[, 1:n_pcs[n]])\n    # Projection of face with mean subtracted onto selected Eigenvectors\n    proj &lt;- t(vector_removed_n) %*% evecs_selected\n    # Reconstructed vector from projection and Eigenvectors with mean added back\n    vector_reconstr &lt;- t(proj %*% t(evecs_selected)) + vector_mean\n\n    # Rasterize and show reconstructed face\n    face_reconstr &lt;- create_face(vector_reconstr)\n    show_face_grayscale(face_reconstr)\n    title(paste0(n_pcs[n], \" PCs\"))\n}\n\n\n\n\n\n\n\nWe can see clearly that the more principal components used in the reconstruction, the better the reconstruction is, and the closer it is to the original raw face. With only the top 10 principal components, the face reconstruction looks very similar to the mean. With 399 principal components, the reconstruction is virtually identical to the raw."
  },
  {
    "objectID": "pages/filters.html#initial-setup",
    "href": "pages/filters.html#initial-setup",
    "title": "Edge and Corner Detection",
    "section": "",
    "text": "# load libraries\nsuppressPackageStartupMessages({\n    library(imager)\n    library(ggplot2)\n    library(patchwork)\n})\n\n# load image from file into R as a cimg object\nimg &lt;- load.image(\"../data/image1.png\")\nclass(img)\n\n[1] \"cimg\"         \"imager_array\" \"numeric\"     \n\n\nUsing the imager library, all cimg objects are represented as 4-dimensional arrays. The dimensions represent: width, height, depth, and colour. In our case, our input image is already in grayscale, and we are only concerned with each pixel value, thus the dimensions for depth and colour are largely ignored. Nonetheless, we have to initialize our image as well as all subsequent filters and images in our code to contain all four dimensions so that they will be able to convolve.\n\n# see dimensions of image\ndim(img)\n\n[1] 276 182   1   1\n\n# view summary of image data\nimg\n\nImage. Width: 276 pix Height: 182 pix Depth: 1 Colour channels: 1 \n\n\n\n# helper function applies convolution to an image given a filter\napply_convolution &lt;- function(image, filter) {\n    # Get dimensions of the image and the filter\n    img_dim &lt;- dim(image)\n    filt_dim &lt;- dim(filter)\n\n    # Define padding size\n    padding &lt;- floor(filt_dim[1] / 2)\n\n    # initialize padded image as empty array\n    img_padded &lt;- array(0, dim = c(\n        img_dim[1] + 2 * padding, img_dim[2] + 2 * padding, img_dim[3], img_dim[4]\n    ))\n\n    # populate padded image array with image pixels\n    img_padded[\n        (1 + padding):(img_dim[1] + padding), # width\n        (1 + padding):(img_dim[2] + padding), , # height\n    ] &lt;- image\n\n    # initialize convolved image as empty array\n    img_convolved &lt;- array(0, dim = img_dim)\n\n    # iterate each pixel in image and convolve with filter\n    for (i in 1:img_dim[1]) {\n        for (j in 1:img_dim[2]) {\n            pixel_convolved &lt;- 0\n            for (k in 1:filt_dim[1]) {\n                for (l in 1:filt_dim[2]) {\n                    pixel_convolved &lt;- pixel_convolved +\n                        filter[k, l, , ] *\n                            img_padded[i + k - 1, j + l - 1, , ]\n                }\n            }\n            img_convolved[i, j, , ] &lt;- pixel_convolved\n        }\n    }\n\n    return(as.cimg(img_convolved))\n}\n\n\n# helper function displays images, given an image and title\nplot_img &lt;- function(img, title = \"\") {\n    # convert image to dataframe\n    img_df &lt;- as.data.frame(img)\n\n    # generate plot of dataframe\n    plot &lt;- ggplot(img_df, aes(x, y)) + # nolint\n        # define rasterization\n        geom_raster(aes(fill = value), show.legend = FALSE) + # nolint\n        # define color scale (grayscale)\n        scale_fill_gradient(low = \"black\", high = \"white\") +\n        # fix pixel ratio / aspect ratio\n        coord_fixed() +\n        # reverse y scale (image orientation)\n        scale_y_reverse() +\n        # set title\n        ggtitle(title) +\n        # remove all graphical details from plot besides image\n        theme_void()\n\n    return(plot)\n}\n\n# display the original image\nplot_img(img, \"Original Image\")"
  },
  {
    "objectID": "pages/filters.html#a",
    "href": "pages/filters.html#a",
    "title": "Edge and Corner Detection",
    "section": "",
    "text": "Implement convolution process of smoothing Image 1 with a 5 x 5 Gaussian Filter with σ = 1 and 2, plot the corresponding output images.\n\n# function applies a kxk Gaussian filter to an image with a sigma value\napply_gaussian &lt;- function(img, ksize, sigma) {\n    # initialize x and y vectors around 0, e.g. [-2, -1, 0, 1, 2]\n    center &lt;- floor(ksize / 2)\n    x &lt;- y &lt;- seq(-center, center, 1)\n\n    # Gaussian function\n    gauss &lt;- outer(x, y, function(x, y) {\n        exp(-(x^2 + y^2) / (2 * sigma^2))\n    })\n\n    # initialize kernel as empty array\n    kernel &lt;- array(0, dim = c(ksize, ksize, 1, 1))\n    # populate kernel with Gaussian filter values\n    kernel[, , 1, 1] &lt;- gauss\n\n    # display kernel values\n    print(\"Resulting kernel:\")\n    print(kernel[, , 1, 1])\n\n    # convolve image with kernel\n    img_flt &lt;- apply_convolution(img, kernel)\n    return(img_flt)\n}\n\n# apply Gaussian filter to image with sigma = 1\nimg_gauss_1 &lt;- apply_gaussian(img, ksize = 5, sigma = 1)\n\n[1] \"Resulting kernel:\"\n           [,1]      [,2]      [,3]      [,4]       [,5]\n[1,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n[2,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[3,] 0.13533528 0.6065307 1.0000000 0.6065307 0.13533528\n[4,] 0.08208500 0.3678794 0.6065307 0.3678794 0.08208500\n[5,] 0.01831564 0.0820850 0.1353353 0.0820850 0.01831564\n\n# apply Gaussian filter to image with sigma = 2\nimg_gauss_2 &lt;- apply_gaussian(img, ksize = 5, sigma = 2)\n\n[1] \"Resulting kernel:\"\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 0.3678794 0.5352614 0.6065307 0.5352614 0.3678794\n[2,] 0.5352614 0.7788008 0.8824969 0.7788008 0.5352614\n[3,] 0.6065307 0.8824969 1.0000000 0.8824969 0.6065307\n[4,] 0.5352614 0.7788008 0.8824969 0.7788008 0.5352614\n[5,] 0.3678794 0.5352614 0.6065307 0.5352614 0.3678794\n\n# show resulting images side by side\nplot_img(img_gauss_1, \"Sigma = 1\") + plot_img(img_gauss_2, \"Sigma = 2\")\n\n\n\n\n\n\n\n\nWe can see that the blurring effect from the Gaussian filter is stronger with a greater sigma value."
  },
  {
    "objectID": "pages/filters.html#b",
    "href": "pages/filters.html#b",
    "title": "Edge and Corner Detection",
    "section": "",
    "text": "Implement convolution process of convolving a 3 x 3 Sobel filters (Sx and Sy) with the output images you obtained in step a. Plot the outputs of this process.\n\n# define Sobel kernel Sx\nsobel_x &lt;- array(\n    c(\n        -1, 0, 1,\n        -2, 0, 2,\n        -1, 0, 1\n    ),\n    dim = c(3, 3, 1, 1)\n)\n\n# define Sobel kernel Sy\nsobel_y &lt;- array(\n    c(\n        -1, -2, -1,\n        0, 0, 0,\n        1, 2, 1\n    ),\n    dim = c(3, 3, 1, 1)\n)\n\nApplying Sobel filters to Image with Gaussian filter where σ = 1:\n\n# apply Sobel filters to first Gaussian filter output\nimg_g1_sx &lt;- apply_convolution(img_gauss_1, sobel_x) # x filter\nimg_g1_sy &lt;- apply_convolution(img_gauss_1, sobel_y) # y filter\n\n# show resulting images side by side\nplot_img(img_g1_sx, \"Sigma = 1, Sobel x\") +\n    plot_img(img_g1_sy, \"Sigma = 1, Sobel y\")\n\n\n\n\n\n\n\n\nApplying Sobel filters to Image with Gaussian filter where σ = 2:\n\n# apply Sobel filters to second Gaussian filter output\nimg_g2_sx &lt;- apply_convolution(img_gauss_2, sobel_x) # x filter\nimg_g2_sy &lt;- apply_convolution(img_gauss_2, sobel_y) # y filter\n\n# Show resulting images side by side\nplot_img(img_g2_sx, \"Sigma = 2, Sobel x\") +\n    plot_img(img_g2_sy, \"Sigma = 2, Sobel y\")\n\n\n\n\n\n\n\n\nEdge detection is a bit sharper with the lower sigma value, however this comes at the cost of increased noise."
  },
  {
    "objectID": "pages/filters.html#c",
    "href": "pages/filters.html#c",
    "title": "Edge and Corner Detection",
    "section": "",
    "text": "Implement convolution process of convolving Image 1 with \\(\\frac{\\partial G}{\\partial x}\\) and \\(\\frac{\\partial G}{\\partial y}\\) filters (5 x 5 filters and for σ = 1 and 2).\n\n# function applies first partial x-derivative Gaussian filter to an image\napply_d_gaussian_x &lt;- function(img, ksize, sigma) {\n    # initialize x and y vectors around 0, e.g. [-2, -1, 0, 1, 2]\n    center &lt;- floor(ksize / 2)\n    x &lt;- y &lt;- seq(-center, center, 1)\n\n    # first derivative of Gaussian function wrt x\n    dgauss &lt;- outer(x, y, function(x, y) {\n        (-x / sigma^2) * exp(-(x^2 + y^2) / (2 * sigma^2))\n    })\n\n    # initialize kernel as empty array\n    kernel &lt;- array(0, dim = c(ksize, ksize, 1, 1))\n    # populate kernel with x-derivative Gaussian filter values\n    kernel[, , 1, 1] &lt;- dgauss\n\n    # display kernel values\n    print(\"Resulting kernel:\")\n    print(kernel[, , 1, 1])\n\n    # convolve image with kernel window\n    img_flt &lt;- apply_convolution(img, kernel)\n    return(img_flt)\n}\n\n# function applies first partial y-derivative Gaussian filter to an image\napply_d_gaussian_y &lt;- function(img, ksize, sigma) {\n    # initialize x and y vectors around 0, e.g. [-2, -1, 0, 1, 2]\n    center &lt;- floor(ksize / 2)\n    x &lt;- y &lt;- seq(-center, center, 1)\n\n    # first derivative of Gaussian function wrt y\n    dgauss &lt;- outer(x, y, function(x, y) {\n        (-y / sigma^2) * exp(-(x^2 + y^2) / (2 * sigma^2))\n    })\n\n    # initialize kernel as empty array\n    kernel &lt;- array(0, dim = c(ksize, ksize, 1, 1))\n    # populate kernel with y-derivative Gaussian filter values\n    kernel[, , 1, 1] &lt;- dgauss\n\n    # display kernel values\n    print(\"Resulting kernel:\")\n    print(kernel[, , 1, 1])\n\n    # convolve image with kernel window\n    img_flt &lt;- apply_convolution(img, kernel)\n    return(img_flt)\n}\n\n# apply derivative Gaussian filters to Image with sigma = 1\nimg_dg1_x &lt;- apply_d_gaussian_x(img, ksize = 5, sigma = 1) # x-derivative\n\n[1] \"Resulting kernel:\"\n            [,1]       [,2]       [,3]       [,4]        [,5]\n[1,]  0.03663128  0.1641700  0.2706706  0.1641700  0.03663128\n[2,]  0.08208500  0.3678794  0.6065307  0.3678794  0.08208500\n[3,]  0.00000000  0.0000000  0.0000000  0.0000000  0.00000000\n[4,] -0.08208500 -0.3678794 -0.6065307 -0.3678794 -0.08208500\n[5,] -0.03663128 -0.1641700 -0.2706706 -0.1641700 -0.03663128\n\nimg_dg1_y &lt;- apply_d_gaussian_y(img, ksize = 5, sigma = 1) # y-derivative\n\n[1] \"Resulting kernel:\"\n           [,1]      [,2] [,3]       [,4]        [,5]\n[1,] 0.03663128 0.0820850    0 -0.0820850 -0.03663128\n[2,] 0.16417000 0.3678794    0 -0.3678794 -0.16417000\n[3,] 0.27067057 0.6065307    0 -0.6065307 -0.27067057\n[4,] 0.16417000 0.3678794    0 -0.3678794 -0.16417000\n[5,] 0.03663128 0.0820850    0 -0.0820850 -0.03663128\n\n# apply derivative Gaussian filters to Image with sigma = 2\nimg_dg2_x &lt;- apply_d_gaussian_x(img, ksize = 5, sigma = 2) # x-derivative\n\n[1] \"Resulting kernel:\"\n           [,1]       [,2]       [,3]       [,4]       [,5]\n[1,]  0.1839397  0.2676307  0.3032653  0.2676307  0.1839397\n[2,]  0.1338154  0.1947002  0.2206242  0.1947002  0.1338154\n[3,]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000\n[4,] -0.1338154 -0.1947002 -0.2206242 -0.1947002 -0.1338154\n[5,] -0.1839397 -0.2676307 -0.3032653 -0.2676307 -0.1839397\n\nimg_dg2_y &lt;- apply_d_gaussian_y(img, ksize = 5, sigma = 2) # y-derivative\n\n[1] \"Resulting kernel:\"\n          [,1]      [,2] [,3]       [,4]       [,5]\n[1,] 0.1839397 0.1338154    0 -0.1338154 -0.1839397\n[2,] 0.2676307 0.1947002    0 -0.1947002 -0.2676307\n[3,] 0.3032653 0.2206242    0 -0.2206242 -0.3032653\n[4,] 0.2676307 0.1947002    0 -0.1947002 -0.2676307\n[5,] 0.1839397 0.1338154    0 -0.1338154 -0.1839397\n\n# Show resulting images\nplot_img(img_dg1_x, \"Sigma = 1, Derivative Gaussian x\") +\n    plot_img(img_dg1_y, \"Sigma = 1, Derivative Gaussian y\")\n\n\n\n\n\n\n\nplot_img(img_dg2_x, \"Sigma = 2, Derivative Gaussian x\") +\n    plot_img(img_dg2_y, \"Sigma = 2, Derivative Gaussian y\")\n\n\n\n\n\n\n\n\n\n\n\nStep b utilized edge detection using the Sobel filter for x and y, and step c utilized the first derivative Gaussian filter for x and y. We can plot a review of the output images below:\n\n# plot images generated from sigma = 1\nplot_img(img_g1_sx, \"Sigma = 1, Sobel x\") +\n    plot_img(img_g1_sy, \"Sigma = 1, Sobel y\") +\n    plot_img(img_dg1_x, \"Sigma = 1, Derivative Gaussian x\") +\n    plot_img(img_dg1_y, \"Sigma = 1, Derivative Gaussian y\") +\n    plot_layout(ncol = 2)\n\n\n\n\n\n\n\n# plot images generated from sigma = 2\nplot_img(img_g2_sx, \"Sigma = 2, Sobel x\") +\n    plot_img(img_g2_sy, \"Sigma = 2, Sobel y\") +\n    plot_img(img_dg2_x, \"Sigma = 2, Derivative Gaussian x\") +\n    plot_img(img_dg2_y, \"Sigma = 2, Derivative Gaussian y\") +\n    plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\nFrom the rasterizations above, we can see that both filters are effective at their main goal of edge detection. On it’s own, the Sobel filters is known to be sensitive to noise, and so we first filtered the image using Gaussian filters before applying the Sobel filters. The first derivative Gaussian filter does not have as much sensitivity. One noticeable difference is the opposite visual effects the filters have; the edges that appear to be protruding in the Sobel filter, appear to be sunken in the derivative Gaussian filter. This likely results from how the two filters emphasize the orientation of edges, with opposite negative and positive values in the resulting image. Finally, another difference observed between the two filters is that the edges are lighter and thicker with the Sobel filters, whilst the edges are darker and sharper with the derivative Gaussian filter (albeit to a slim extent). This may be attributed to the extra step of pre-filtering the image with a Gaussian filter before applying the Sobel filter, which adds an extra level of blur. On the other hand, the derivative Gaussian filter both removes noise and detects edges in one filter, which removes an extra step needed, also resulting in less of a blurring effect."
  },
  {
    "objectID": "pages/linreg.html#second-order-polynomial",
    "href": "pages/linreg.html#second-order-polynomial",
    "title": "Simple Regression",
    "section": "Second Order Polynomial",
    "text": "Second Order Polynomial\nImplement the 2nd-order polynomial regression by adding new features x2 to the inputs. Repeat (b) and (c). Compare the training error and test error. Is it a better fit than linear regression?\n\n# add second order polynomial to features\nx_tr_2 &lt;- as.matrix(x_tr_1[, 1]**2)\nx_tr_2 &lt;- cbind(x_tr_2, x_tr_1)\n\n# compute weight vector\nw2 &lt;- solve(t(x_tr_2) %*% x_tr_2) %*% (t(x_tr_2) %*% y_tr_mat)\n\n# plot polynomial regression on training set\nplt_tr_2 &lt;- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w2[1] * x^2) + (w2[2] * x) + w2[3]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_2\n\n\n\n# compute average training error\neq_tr_2 &lt;- t(as.data.frame(t(w2) %*% t(x_tr_2)))\nerrors_tr_2 &lt;- (eq_tr_2 - y_tr_mat)**2\nerr_avg_tr_2 &lt;- colSums(errors_tr_2) / length(errors_tr_2)\nerr_avg_tr_2\n\n   y_tr \n1.98476 \n\n# add columns to testing features\nx_te_2 &lt;- as.matrix(x_te_1[, 1]**2)\nx_te_2 &lt;- cbind(x_te_2, x_te_1)\n\n# plot polynomial regression on test set\nplt_te_2 &lt;- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w2[1] * x^2) + (w2[2] * x) + w2[3]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_2\n\n\n\n# compute average testing error using same weight vector from training data\neq_te_2 &lt;- t(as.data.frame(t(w2) %*% t(x_te_2)))\nerrors_te_2 &lt;- (eq_te_2 - y_te_mat)**2\nerr_avg_te_2 &lt;- colSums(errors_te_2) / length(errors_te_2)\nerr_avg_te_2\n\n    y_tr \n12.16348 \n\n# store average errors\nerrs &lt;- rbind(errs, order2 = c(err_avg_tr_2, err_avg_te_2))\nerrs\n\n       training   testing\norder1 2.143112  9.973294\norder2 1.984760 12.163477\n\n\nComparing the training error and test error, this second order polynomial regression model performs worse than the linear model, with a better training fit but a worse testing fit. Thus, the linear regression model is a better fit."
  },
  {
    "objectID": "pages/linreg.html#third-order-polynomial",
    "href": "pages/linreg.html#third-order-polynomial",
    "title": "Simple Regression",
    "section": "Third Order Polynomial",
    "text": "Third Order Polynomial\nImplement the 3rd-order polynomial regression by adding new features x2,x3 to the inputs. Repeat (b) and (c). Compare the training error and test error. Is it a better fit than linear regression and 2nd-order polynomial regression?\n\n# add third order polynomial to features\nx_tr_3 &lt;- as.matrix(x_tr_1[, 1]**3)\nx_tr_3 &lt;- cbind(x_tr_3, x_tr_2)\n\n# compute weight vector\nw3 &lt;- solve(t(x_tr_3) %*% x_tr_3) %*% (t(x_tr_3) %*% y_tr_mat)\n\n# plot polynomial regression on training set\nplt_tr_3 &lt;- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w3[1] * x^3) + (w3[2] * x^2) + (w3[3] * x) + w3[4]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_3\n\n\n\n# compute average training error\neq_tr_3 &lt;- t(as.data.frame(t(w3) %*% t(x_tr_3)))\nerrors_tr_3 &lt;- (eq_tr_3 - y_tr_mat)**2\nerr_avg_tr_3 &lt;- colSums(errors_tr_3) / length(errors_tr_3)\nerr_avg_tr_3\n\n     y_tr \n0.8850264 \n\n# add columns to testing features\nx_te_3 &lt;- as.matrix(x_te_1[, 1]**3)\nx_te_3 &lt;- cbind(x_te_3, x_te_2)\n\n# plot polynomial regression on test set\nplt_te_3 &lt;- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w3[1] * x^3) + (w3[2] * x^2) + (w3[3] * x) + w3[4]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_3\n\n\n\n# compute average testing error using same weight vector from training data\neq_te_3 &lt;- t(as.data.frame(t(w3) %*% t(x_te_3)))\nerrors_te_3 &lt;- (eq_te_3 - y_te_mat)**2\nerr_avg_te_3 &lt;- colSums(errors_te_3) / length(errors_te_3)\nerr_avg_te_3\n\n    y_tr \n2.715822 \n\n# store average errors\nerrs &lt;- rbind(errs, order3 = c(err_avg_tr_3, err_avg_te_3))\nerrs\n\n        training   testing\norder1 2.1431122  9.973294\norder2 1.9847596 12.163477\norder3 0.8850264  2.715822\n\n\nThis time, the training error has improved much more than with the second order model. In addition, the testing error has also dramatically improved, even better than our original best linear regression model. Overall, this third order polynomial regression is a better fit than both the linear and second order models."
  },
  {
    "objectID": "pages/linreg.html#fourth-order-polynomial",
    "href": "pages/linreg.html#fourth-order-polynomial",
    "title": "Simple Regression",
    "section": "Fourth Order Polynomial",
    "text": "Fourth Order Polynomial\nImplement the 4th-order polynomial regression by adding new features x2,x3,x4 to the inputs. Repeat (b) and (c). Compare the training error and test error. Compared with the previous results, which order is the best for fitting the data?\n\n# add fourth order polynomial to features\nx_tr_4 &lt;- as.matrix(x_tr_1[, 1]**4)\nx_tr_4 &lt;- cbind(x_tr_4, x_tr_3)\n\n# compute weight vector\nw4 &lt;- solve(t(x_tr_4) %*% x_tr_4) %*% (t(x_tr_4) %*% y_tr_mat)\neq_tr_4 &lt;- t(as.data.frame(t(w4) %*% t(x_tr_4)))\n\n# plot polynomial regression on training set\nplt_tr_4 &lt;- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_4\n\n\n\n# compute average training error\nerrors_tr_4 &lt;- (eq_tr_4 - y_tr_mat)**2\nerr_avg_tr_4 &lt;- colSums(errors_tr_4) / length(errors_tr_4)\nerr_avg_tr_4\n\n     y_tr \n0.8833612 \n\n# add columns to testing features\nx_te_4 &lt;- as.matrix(x_te_1[, 1]**4)\nx_te_4 &lt;- cbind(x_te_4, x_te_3)\n\n# equation using same weight vector from training data\neq_te_4 &lt;- t(as.data.frame(t(w4) %*% t(x_te_4)))\n\n# plot polynomial regression on test set\nplt_te_4 &lt;- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_4\n\n\n\n# compute average testing error using same weight vector from training data\nerrors_te_4 &lt;- (eq_te_4 - y_te_mat)**2\nerr_avg_te_4 &lt;- colSums(errors_te_4) / length(errors_te_4)\nerr_avg_te_4\n\n   y_tr \n3.05486 \n\n# store average errors\nerrs &lt;- rbind(errs, order4 = c(err_avg_tr_4, err_avg_te_4))\nerrs\n\n        training   testing\norder1 2.1431122  9.973294\norder2 1.9847596 12.163477\norder3 0.8850264  2.715822\norder4 0.8833612  3.054860\n\n\nUsing fourth order polynomial regression, our training error has slightly improved, however, the testing error has slightly gotten worse, by a larger amount. While it is close to the third order polynomial regression, overall the data has become slightly overfitted, and so the best model for fitting our data is the third order polynomial regression model."
  },
  {
    "objectID": "pages/linreg.html#background",
    "href": "pages/linreg.html#background",
    "title": "Simple Regression",
    "section": "Background",
    "text": "Background\nIn supervised learning, our input variables, also called features, are used in order to predict an output variable, also called a label. There are different types of output variables that we may be interested in, which would fall under the category of regression or classification. In regression problems, we are interested in predicting a continuous output variable, whereas in classification problems, we are interested in predicting a discrete output variable. In this exercise, we will be working with a regression problem, where we are interested in predicting a continuous output variable.\n\n\n\n\n\n\nOutput variable examples\n\n\n\n\n\nIn a classification problem, we might input features based on tumour width and height, and predict whether or not the tumour is malignant or benign. The output variable in this case would be a discrete variable, either 0 or 1, where 0 represents benign and 1 represents malignant.\nIn a regression problem, we might input features based on gene expression levels of several other genes, and predict the gene expression level of a specific gene of interest. The output variable in this case would be a continuous variable, where the gene expression level is represented by a floating point number.\nIn each of these types of problems, the input variables can be anything (continuous or discrete), but the type of output variable will determine whether or not the problem is a regression or classification problem.\n\n\n\n\n\n\n\n\n\nOther supervised learning problems\n\n\n\n\n\nWhile classification and regression are the most commonly seen supervised learning problems, other supervised learning problems include:\n\nRanking: Predicting the order of a set of items.\nSequence labeling: Assigning a label to each element in a sequence of inputs.\nStructured prediction: Predicting a structured output, such as a sequence, tree, or graph."
  },
  {
    "objectID": "pages/regression.html",
    "href": "pages/regression.html",
    "title": "Simple Regression",
    "section": "",
    "text": "Important\n\n\n\nThis page is currently under construction, and being slowly updated with better explanations and formatting."
  },
  {
    "objectID": "pages/regression.html#background",
    "href": "pages/regression.html#background",
    "title": "Simple Regression",
    "section": "Background",
    "text": "Background\nIn supervised learning, our input variables, also called features, are used in order to predict an output variable, also called a label. There are different types of output variables that we may be interested in, which would fall under the category of regression or classification. In regression problems, we are interested in predicting a continuous output variable, whereas in classification problems, we are interested in predicting a discrete output variable. In this exercise, we will be working with a regression problem, where we are interested in predicting a continuous output variable.\n\n\n\n\n\n\nOutput variable examples\n\n\n\n\n\nIn a classification problem, we might input features based on tumour width and height, and predict whether or not the tumour is malignant or benign. The output variable in this case would be a discrete variable, either 0 or 1, where 0 represents benign and 1 represents malignant.\nIn a regression problem, we might input features based on gene expression levels of several other genes, and predict the gene expression level of a specific gene of interest. The output variable in this case would be a continuous variable, where the gene expression level is represented by a floating point number.\nIn each of these types of problems, the input variables can be anything (continuous or discrete), but the type of output variable will determine whether or not the problem is a regression or classification problem.\n\n\n\n\n\n\n\n\n\nOther supervised learning problems\n\n\n\n\n\nWhile classification and regression are the most commonly seen supervised learning problems, other supervised learning problems include:\n\nRanking: Predicting the order of a set of items.\nSequence labeling: Assigning a label to each element in a sequence of inputs.\nStructured prediction: Predicting a structured output, such as a sequence, tree, or graph."
  },
  {
    "objectID": "pages/regression.html#model-solving",
    "href": "pages/regression.html#model-solving",
    "title": "Simple Regression",
    "section": "Model Solving",
    "text": "Model Solving\nWe’ll work with the training data only for now, as once we have a weights vector and a completed model, we can then use the testing data to evaluate the model. To relate our data to the linear regression model, we first have to add the column vector of 1’s to the features to account for the bias term.\n\n# Add column of 1's to training features\n1x_tr_1 &lt;- cbind(\n2    x_tr,\n3    b = rep(1, length(x_tr))\n)\n\n\n1\n\nCreate a new variable, x_tr_1, that is the result of the cbind function. This function takes vectors (or dataframes) and combines them by their columns, hence the name cbind for column bind. We will want to combine our original features with a column of 1’s of the same length.\n\n2\n\nSpecify our first vector to combine, x_tr. This is the original training features vector.\n\n3\n\nThe second vector to combine will be a column containing just ones of the same length as our orignal vector. We start with b = to name our column consistently with the formula. We can create this column of 1’s on the fly (instead of manually) by using the rep function. The rep function takes a value and repeats it a specified number of times. In this case, we want to repeat the value 1, length(x_tr) times. The length function returns the length of a vector, in this case the number of elements in x_tr.\n\n\n\n\nNow x_tr_1 looks like this:\n\nhead(x_tr_1)\n\n       x_tr b\n1 3.1381904 1\n2 3.4421490 1\n3 1.3083408 1\n4 0.9954419 1\n5 3.2414549 1\n6 2.8537392 1\n\n\n\n\n\n\n\n\nLike with our _tr suffix, I use the _1 suffix to represent variables for linear regression. Later on I’ll be using other regression models such as _2 for second-order polynomial regression, _3 for third-order, etc.\n\n\n\nNow that we have our training features with the bias term included, we can use the training labels to solve for the weights vector by rearranging the linear regression formula. Using matrix algebra, we derive the formula for solving for the weights vector:\n\\[w = (X^\\top X)^{-1} X^\\top y\\]\nwhere \\(X\\) is the matrix form of training features, \\(y\\) is the vector of training labels, and \\(w\\) is the vector of weights. The \\(X^\\top\\) notation is the transpose of \\(X\\), and the \\(X^{-1}\\) notation is the inverse of \\(X\\). The inverse of a matrix is a matrix that when multiplied by the original matrix, results in the identity matrix. The identity matrix is a square matrix with 1’s on the diagonal and 0’s everywhere else. The identity matrix is the multiplicative identity, meaning that when multiplied by any matrix, the result is the original matrix. The inverse of a matrix is the multiplicative inverse, meaning that when multiplied by the original matrix, the result is the identity matrix.\nMatrix operations are available in base R, including %*% for matrix multiplication, t for transposing a matrix, and solve for solving a matrix equation. When solve is only provided one parameter, it solves for the inverse of the input matrix. We can use these functions to solve for the weights vector.\n\n# Convert training dataframes to matrices\n1y_tr_mat &lt;- as.matrix(y_tr)\nx_tr_1 &lt;- as.matrix(x_tr_1)\n\n# Solve for weight vector using training data\n2w &lt;- solve(t(x_tr_1) %*% x_tr_1) %*% (t(x_tr_1) %*% y_tr_mat)\n\n\n1\n\nTo perform matrix operations on our dataframes, we need to first convert them to matrices. We can do this using the as.matrix function. I rename y_tr to y_tr_mat to store its matrix variable separately, since it will be reused later on in the code.\n\n2\n\nSolve for w using the formula above, with x_tr_1 as X and y_tr_mat as y. We use the %*% operator for matrix multiplication, the solve function to solve for the inverse of a matrix, and the t function to transpose a matrix. The brackets are there to visually support the order of operations.\n\n\n\n\nNow that we have our weights vector \\(w\\), the full linear regression formula is complete and we can plot the resulting function on top of our training data.\n\n# Plot linear regression on training set\n1plt_tr_1 &lt;- ggplot(\n2    data = df_tr,\n3    mapping = aes(x = x_tr, y = y_tr)\n) +\n4    geom_point() +\n5    stat_function(fun = function(x) {\n        w[1] * x + w[2]\n    }) +\n6    xlim(0, 4) +\n    ylim(0, 30) +\n7    theme_light()\n\n8plt_tr_1\n\n\n1\n\nCreate a ggplot object for the linear regression over the training data.\n\n2\n\nParameter data specifies the dataframe to use for the plot. We’re using the training data dataframe.\n\n3\n\nParameter mapping specifies the mapping of the dataframe columns to the plot axes, i.e. what our x variable and y variable are in terms of column names. These go in the aes function, for specifying aesthetics of the plot.\n\n4\n\nSpecify the plot type; geom_point is a basic scatter plot, ideal for plotting individual data points. Using geom_scatter could also work here.\n\n5\n\nSpecify another plot type (alongside geom_point). Additional plot types can be added to render on top of the main data (the training data). Here we are plotting a function using stat_function by passing in the function as paramter fun. The function is created in y = mx + b format from our weights vector w, so function of x is w[1] * x + w[2] where w[1] is the slope m and w[2] is the y-intercept b. The function is plotted over the range of the training data, which is why we don’t need to specify x in the stat_function function.\n\n6\n\nSet the x and y axis limits. This is optional, but I’m specifying them to manually the plots limits rather than auto-detecting.\n\n7\n\nSet the plot theme.\n\n8\n\nDisplay the plot."
  },
  {
    "objectID": "pages/regression.html#average-error",
    "href": "pages/regression.html#average-error",
    "title": "Simple Regression",
    "section": "Average Error",
    "text": "Average Error\n\n# compute average training error\neq_tr_1 &lt;- t(as.data.frame(t(w) %*% t(x_tr_1)))\nerrors_tr_1 &lt;- (eq_tr_1 - y_tr_mat)**2\nerr_avg_tr_1 &lt;- colSums(errors_tr_1) / length(errors_tr_1)\nerr_avg_tr_1\n\n    y_tr \n2.143112 \n\n\nThe average error of the training set using the linear regression model is 2.287073.\nPlot both the regression line and the test data on the same graph. Also report the average error on the test set using Eq. (1).\n\n# convert training outputs to matrix\ny_te_mat &lt;- as.matrix(y_te)\n\n# add column of 1s to testing features\nx_te_1 &lt;- cbind(x_te, b = rep(1, length(x_te)))\nx_te_1 &lt;- as.matrix(x_te_1)\n\n# plot linear regression on test set\nplt_te_1 &lt;- ggplot(\n    data = df_te,\n    mapping = aes(x = x_te, y = y_te)\n) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        w[1] * x + w[2]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_1\n\n\n\n# compute average test error using same weight vector from training data\neq_te_1 &lt;- t(as.data.frame(t(w) %*% t(x_te_1)))\nerrors_te_1 &lt;- (eq_te_1 - y_te_mat)**2\nerr_avg_te_1 &lt;- colSums(errors_te_1) / length(errors_te_1)\nerr_avg_te_1\n\n    y_tr \n9.973294 \n\n# store average errors:\nerrs &lt;- data.frame(\n    \"training\" = err_avg_tr_1,\n    \"testing\" = err_avg_te_1,\n    row.names = \"order1\"\n)\n\nThe average error of the testing set using the linear regression model is 9.427358."
  },
  {
    "objectID": "pages/regression.html#second-order-polynomial",
    "href": "pages/regression.html#second-order-polynomial",
    "title": "Simple Regression",
    "section": "Second Order Polynomial",
    "text": "Second Order Polynomial\nImplement the 2nd-order polynomial regression by adding new features x2 to the inputs. Repeat (b) and (c). Compare the training error and test error. Is it a better fit than linear regression?\n\n# add second order polynomial to features\nx_tr_2 &lt;- as.matrix(x_tr_1[, 1]**2)\nx_tr_2 &lt;- cbind(x_tr_2, x_tr_1)\n\n# compute weight vector\nw2 &lt;- solve(t(x_tr_2) %*% x_tr_2) %*% (t(x_tr_2) %*% y_tr_mat)\n\n# plot polynomial regression on training set\nplt_tr_2 &lt;- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w2[1] * x^2) + (w2[2] * x) + w2[3]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_2\n\n\n\n# compute average training error\neq_tr_2 &lt;- t(as.data.frame(t(w2) %*% t(x_tr_2)))\nerrors_tr_2 &lt;- (eq_tr_2 - y_tr_mat)**2\nerr_avg_tr_2 &lt;- colSums(errors_tr_2) / length(errors_tr_2)\nerr_avg_tr_2\n\n   y_tr \n1.98476 \n\n# add columns to testing features\nx_te_2 &lt;- as.matrix(x_te_1[, 1]**2)\nx_te_2 &lt;- cbind(x_te_2, x_te_1)\n\n# plot polynomial regression on test set\nplt_te_2 &lt;- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w2[1] * x^2) + (w2[2] * x) + w2[3]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_2\n\n\n\n# compute average testing error using same weight vector from training data\neq_te_2 &lt;- t(as.data.frame(t(w2) %*% t(x_te_2)))\nerrors_te_2 &lt;- (eq_te_2 - y_te_mat)**2\nerr_avg_te_2 &lt;- colSums(errors_te_2) / length(errors_te_2)\nerr_avg_te_2\n\n    y_tr \n12.16348 \n\n# store average errors\nerrs &lt;- rbind(errs, order2 = c(err_avg_tr_2, err_avg_te_2))\nerrs\n\n       training   testing\norder1 2.143112  9.973294\norder2 1.984760 12.163477\n\n\nComparing the training error and test error, this second order polynomial regression model performs worse than the linear model, with a better training fit but a worse testing fit. Thus, the linear regression model is a better fit."
  },
  {
    "objectID": "pages/regression.html#third-order-polynomial",
    "href": "pages/regression.html#third-order-polynomial",
    "title": "Simple Regression",
    "section": "Third Order Polynomial",
    "text": "Third Order Polynomial\nImplement the 3rd-order polynomial regression by adding new features x2,x3 to the inputs. Repeat (b) and (c). Compare the training error and test error. Is it a better fit than linear regression and 2nd-order polynomial regression?\n\n# add third order polynomial to features\nx_tr_3 &lt;- as.matrix(x_tr_1[, 1]**3)\nx_tr_3 &lt;- cbind(x_tr_3, x_tr_2)\n\n# compute weight vector\nw3 &lt;- solve(t(x_tr_3) %*% x_tr_3) %*% (t(x_tr_3) %*% y_tr_mat)\n\n# plot polynomial regression on training set\nplt_tr_3 &lt;- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w3[1] * x^3) + (w3[2] * x^2) + (w3[3] * x) + w3[4]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_3\n\n\n\n# compute average training error\neq_tr_3 &lt;- t(as.data.frame(t(w3) %*% t(x_tr_3)))\nerrors_tr_3 &lt;- (eq_tr_3 - y_tr_mat)**2\nerr_avg_tr_3 &lt;- colSums(errors_tr_3) / length(errors_tr_3)\nerr_avg_tr_3\n\n     y_tr \n0.8850264 \n\n# add columns to testing features\nx_te_3 &lt;- as.matrix(x_te_1[, 1]**3)\nx_te_3 &lt;- cbind(x_te_3, x_te_2)\n\n# plot polynomial regression on test set\nplt_te_3 &lt;- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w3[1] * x^3) + (w3[2] * x^2) + (w3[3] * x) + w3[4]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_3\n\n\n\n# compute average testing error using same weight vector from training data\neq_te_3 &lt;- t(as.data.frame(t(w3) %*% t(x_te_3)))\nerrors_te_3 &lt;- (eq_te_3 - y_te_mat)**2\nerr_avg_te_3 &lt;- colSums(errors_te_3) / length(errors_te_3)\nerr_avg_te_3\n\n    y_tr \n2.715822 \n\n# store average errors\nerrs &lt;- rbind(errs, order3 = c(err_avg_tr_3, err_avg_te_3))\nerrs\n\n        training   testing\norder1 2.1431122  9.973294\norder2 1.9847596 12.163477\norder3 0.8850264  2.715822\n\n\nThis time, the training error has improved much more than with the second order model. In addition, the testing error has also dramatically improved, even better than our original best linear regression model. Overall, this third order polynomial regression is a better fit than both the linear and second order models."
  },
  {
    "objectID": "pages/regression.html#fourth-order-polynomial",
    "href": "pages/regression.html#fourth-order-polynomial",
    "title": "Simple Regression",
    "section": "Fourth Order Polynomial",
    "text": "Fourth Order Polynomial\nImplement the 4th-order polynomial regression by adding new features x2,x3,x4 to the inputs. Repeat (b) and (c). Compare the training error and test error. Compared with the previous results, which order is the best for fitting the data?\n\n# add fourth order polynomial to features\nx_tr_4 &lt;- as.matrix(x_tr_1[, 1]**4)\nx_tr_4 &lt;- cbind(x_tr_4, x_tr_3)\n\n# compute weight vector\nw4 &lt;- solve(t(x_tr_4) %*% x_tr_4) %*% (t(x_tr_4) %*% y_tr_mat)\neq_tr_4 &lt;- t(as.data.frame(t(w4) %*% t(x_tr_4)))\n\n# plot polynomial regression on training set\nplt_tr_4 &lt;- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_4\n\n\n\n# compute average training error\nerrors_tr_4 &lt;- (eq_tr_4 - y_tr_mat)**2\nerr_avg_tr_4 &lt;- colSums(errors_tr_4) / length(errors_tr_4)\nerr_avg_tr_4\n\n     y_tr \n0.8833612 \n\n# add columns to testing features\nx_te_4 &lt;- as.matrix(x_te_1[, 1]**4)\nx_te_4 &lt;- cbind(x_te_4, x_te_3)\n\n# equation using same weight vector from training data\neq_te_4 &lt;- t(as.data.frame(t(w4) %*% t(x_te_4)))\n\n# plot polynomial regression on test set\nplt_te_4 &lt;- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_4\n\n\n\n# compute average testing error using same weight vector from training data\nerrors_te_4 &lt;- (eq_te_4 - y_te_mat)**2\nerr_avg_te_4 &lt;- colSums(errors_te_4) / length(errors_te_4)\nerr_avg_te_4\n\n   y_tr \n3.05486 \n\n# store average errors\nerrs &lt;- rbind(errs, order4 = c(err_avg_tr_4, err_avg_te_4))\nerrs\n\n        training   testing\norder1 2.1431122  9.973294\norder2 1.9847596 12.163477\norder3 0.8850264  2.715822\norder4 0.8833612  3.054860\n\n\nUsing fourth order polynomial regression, our training error has slightly improved, however, the testing error has slightly gotten worse, by a larger amount. While it is close to the third order polynomial regression, overall the data has become slightly overfitted, and so the best model for fitting our data is the third order polynomial regression model."
  }
]