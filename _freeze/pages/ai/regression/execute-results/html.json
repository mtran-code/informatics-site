{
  "hash": "ecdc37aa23ce3565f7d01f5fb5636fce",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simple Regression\"\ncode-annotations: hover\nengine: knitr\n---\n\n\n:::{.callout-important title=\"Important\"}\nThis page is currently under construction, and being slowly updated with better explanations and formatting.\n:::\n\n# Overview\n\nIn this exercise, I will be implementing linear and polynomial regression in R using a pre-made training and testing data set. The aim of this page is to make this exercise as simple as possible, so I will not be going into the mathematical details of linear and polynomial regression. I will also provide explanations for as many lines of code as possible, so that you can follow along with the code and understand what is going on.\n\n## Background\n\nIn supervised learning, our input variables, also called *features*, are used in order to predict an output variable, also called a *label*. There are different types of output variables that we may be interested in, which would fall under the category of *regression* or *classification*. In regression problems, we are interested in predicting a continuous output variable, whereas in classification problems, we are interested in predicting a discrete output variable. In this exercise, we will be working with a regression problem, where we are interested in predicting a continuous output variable.\n\n:::{.callout-note title=\"Output variable examples\" collapse=\"true\"}\nIn a **classification** problem, we might input features based on tumour width and height, and predict whether or not the tumour is malignant or benign. The output variable in this case would be a discrete variable, either 0 or 1, where 0 represents benign and 1 represents malignant.\n\nIn a **regression** problem, we might input features based on gene expression levels of several other genes, and predict the gene expression level of a specific gene of interest. The output variable in this case would be a continuous variable, where the gene expression level is represented by a floating point number.\n\nIn each of these types of problems, the input variables can be anything (continuous or discrete), but the type of output variable will determine whether or not the problem is a regression or classification problem.\n:::\n\n:::{.callout-tip title=\"Other supervised learning problems\" collapse=\"true\"}\nWhile classification and regression are the most commonly seen supervised learning problems, other supervised learning problems include:\n\n- **Ranking**: Predicting the order of a set of items.\n- **Sequence labeling**: Assigning a label to each element in a sequence of inputs.\n- **Structured prediction**: Predicting a structured output, such as a sequence, tree, or graph.\n:::\n\n# Loading data\n\nI am working with a relatively small and simple dataset of random datapoints. The data is comprised of a total of 60 datapoints; 40 of which belong to the training set, and 20 of which belong to the testing set. When working in regression problems or other problems involving a training and test set, it is generally advised that you split your data into a ratio of 70-80% training set, and 20-30% testing set; however, for the purposes of this exercise, I will be using a 2/1 ratio. The data is just a simple x and y coordinate, and the goal is to find a line of best fit for the data. In the context of machine learning applications, your x and y coordinates would represent your features and labels, respectively.\n\nYou can see an overview of the dataset I am working with here:\n\n:::{.callout-note title=\"Training Set\" collapse=\"true\" icon=\"false\"}\n| X             | Y             |\n| ------------- | ------------- |\n| 8.0880917e-01 | 1.0073261e+01 |\n| 3.1381904e+00 | 1.5289011e+01 |\n| 3.4421490e+00 | 1.9023864e+01 |\n| 1.3083408e+00 | 9.3499676e+00 |\n| 9.9544190e-01 | 1.0105849e+01 |\n| 3.2414549e+00 | 1.4950901e+01 |\n| 2.8537392e+00 | 1.2890953e+01 |\n| 2.6453137e+00 | 1.1362217e+01 |\n| 2.8419650e+00 | 1.3414665e+01 |\n| 4.5160350e-01 | 7.3284401e+00 |\n| 3.4798880e+00 | 1.7327588e+01 |\n| 1.9057113e+00 | 1.0604819e+01 |\n| 3.2108352e-01 | 5.4250626e+00 |\n| 5.9508185e-01 | 7.9049958e+00 |\n| 1.2960970e+00 | 8.9717206e+00 |\n| 2.7029539e-02 | 1.6097071e+00 |\n| 2.8675203e+00 | 1.4140329e+01 |\n| 2.6030689e-01 | 3.9944650e+00 |\n| 1.6953830e+00 | 1.2292687e+01 |\n| 3.2195184e+00 | 1.3296842e+01 |\n| 1.1798449e-01 | 2.7942758e+00 |\n| 2.1030732e+00 | 1.3165072e+01 |\n| 3.1493441e+00 | 1.6567816e+01 |\n| 3.2475034e+00 | 1.6648354e+01 |\n| 1.3658297e+00 | 1.0988057e+01 |\n| 1.5986934e+00 | 1.0889961e+01 |\n| 3.4493268e+00 | 1.8209443e+01 |\n| 8.5425227e-03 | 2.2775920e+00 |\n| 6.7585068e-01 | 9.1419999e+00 |\n| 1.5345147e+00 | 1.1422151e+01 |\n| 2.3998049e+00 | 1.1915797e+01 |\n| 5.2187583e-01 | 7.1733038e+00 |\n| 1.3868138e-01 | 3.1359945e+00 |\n| 2.8076295e+00 | 1.4299533e+01 |\n| 2.1267568e+00 | 9.8042369e+00 |\n| 2.9937503e+00 | 1.4940644e+01 |\n| 6.0825958e-01 | 7.2362913e+00 |\n| 2.7257011e-01 | 5.4156210e+00 |\n| 1.9525441e+00 | 1.1138174e+01 |\n| 2.1377665e+00 | 1.3405720e+01 |\n:::\n\n::: {.callout-note title=\"Testing Set\" collapse=\"true\" icon=\"false\"}\n| X             | Y             |\n| ------------- | ------------- |\n| 2.3114047e+00 | 1.2759413e+01 |\n| 2.6561232e+00 | 1.2932647e+01 |\n| 1.7697643e+00 | 1.2440984e+01 |\n| 5.5082798e-01 | 8.4365770e+00 |\n| 8.1445064e-01 | 6.6693487e+00 |\n| 8.7971425e-01 | 1.1014746e+01 |\n| 3.1300948e+00 | 1.6114484e+01 |\n| 2.8314420e+00 | 1.3520417e+01 |\n| 2.0959708e+00 | 1.0853339e+01 |\n| 3.9161472e+00 | 2.7639382e+01 |\n| 1.0678487e+00 | 9.7770559e+00 |\n| 2.9134610e+00 | 1.3137343e+01 |\n| 1.4881792e+00 | 1.1732934e+01 |\n| 1.9346990e+00 | 1.2599440e+01 |\n| 3.1029632e+00 | 1.4806041e+01 |\n| 3.7949112e+00 | 2.5947658e+01 |\n| 2.6665423e+00 | 1.1790299e+01 |\n| 2.9375816e+00 | 1.3475470e+01 |\n| 2.5834993e+00 | 1.3853263e+01 |\n| 7.6000092e-02 | 5.2524787e+00 |\n:::\n\nThe data is available in the source repository as `xtr.dat` and `ytr.dat` for the training data, and `xte.dat` and `yte.dat` for the testing data, in the following directory:\n\n::: {.cell}\n\n```{.r .cell-code}\nDATA_DIR <- \"../../data/train-test/\"\n```\n:::\n\n\nThe file extensions (`.dat`) are arbitrary, but the files are space-delimited. The delimiter shouldn't actually matter since the data only contains one column per file, but alternatively the dat file could include both x and y values in the same file, in which case the delimiter would be important.\n\nTo load the data into R, you can use the following code:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in the training dataset file\nx_tr <- read.delim(                         # <1>\n    file = file.path(DATA_DIR, \"xtr.dat\"),  # <2>\n    col.names = \"x_tr\",                     # <3>\n    colClasses = c(\"numeric\")               # <4>\n)                                           # <1>\ny_tr <- read.delim(                         # <5>\n    file = file.path(DATA_DIR, \"ytr.dat\"),  # <5>\n    col.names = \"y_tr\",                     # <5>\n    colClasses = c(\"numeric\")               # <5>\n)                                           # <5>\n# Store the training data into a dataframe\ndf_tr <- data.frame(x_tr, y_tr)             # <6>\n\n# Read in the testing dataset file\nx_te <- read.delim(                         # <7>\n    file = file.path(DATA_DIR, \"xte.dat\"),  # <7>\n    col.names = \"x_te\",                     # <7>\n    colClasses = c(\"numeric\")               # <7>\n)                                           # <7>\ny_te <- read.delim(                         # <8>\n    file = file.path(DATA_DIR, \"yte.dat\"),  # <8>\n    col.names = \"y_te\",                     # <8>\n    colClasses = c(\"numeric\")               # <8>\n)\n# Store the testing data into a dataframe\ndf_te <- data.frame(x_te, y_te)             # <9>\n```\n:::\n\n1. Read in the training data x-values and store the data into variable `x_tr`.\n2. Parameter `file` specifies the path to the file. I'm using the `file.path` function to join the directory path with the file name, giving me the full path to my file.\n3. Parameter `col.names` specifies the name of the column. If I didn't specify this, the column would be named the default, which would end up being something based on the first line of the file, but since we don't have a header this wouldn't be very descriptive.\n4. Parameter `colClasses` specifies the type of the column. This is optional as R should be able to infer the type, but I'm specifying it here to be explicit.\n5. Repeat steps 1-4 for the training data y-values file.\n6. Store the training data into a dataframe, mapping x values to y values.\n7. Repeat steps 1-4 for the testing data x-values file.\n8. Repeat steps 1-4 for the testing data y-values file.\n9. Store the testing data into a dataframe, mapping x values to y values.\n\nHere is what the dataframes look like now (from the first few rows):\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(df_tr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x_tr      y_tr\n1 3.1381904 15.289011\n2 3.4421490 19.023864\n3 1.3083408  9.349968\n4 0.9954419 10.105849\n5 3.2414549 14.950901\n6 2.8537392 12.890953\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(df_te)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x_te      y_te\n1 2.6561232 12.932647\n2 1.7697643 12.440984\n3 0.5508280  8.436577\n4 0.8144506  6.669349\n5 0.8797143 11.014746\n6 3.1300948 16.114484\n```\n\n\n:::\n:::\n\nEach row is a training example (or instance), and the columns represent the x and y coordinates, or features and labels.\n\n:::{.callout-note appearance=\"simple\"}\nIn case you haven't picked up on it yet, `tr` will be a suffix referring to the **tr**aining data, and `te` will be a suffix referring to the **te**sting data.\n:::\n\nNow that we have the data loaded into R, we can plot it to see what it looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup libraries\nlibrary(ggplot2)                        # <1>\nlibrary(patchwork)                      # <1>\n\n# Plot training dataset\nplt_tr <- ggplot(                       # <2>\n    data = df_tr,                       # <3>\n    mapping = aes(x = x_tr, y = y_tr)   # <4>\n) +                                     # <2>\n    geom_point() +                      # <5>\n    xlim(0, 4) +                        # <6>\n    ylim(0, 30) +                       # <6>\n    ggtitle(\"Training\") +               # <7>\n    theme_light()                       # <8>\n\n# Plot test dataset\nplt_te <- ggplot(                       # <9>\n    data = df_te,                       # <9>\n    mapping = aes(x = x_te, y = y_te)   # <9>\n) +                                     # <9>\n    geom_point() +                      # <9>\n    xlim(0, 4) +                        # <9>\n    ylim(0, 30) +                       # <9>\n    ggtitle(\"Testing\") +                # <9>\n    theme_light()                       # <9>\n\nplt_tr + plt_te                         # <10>\n```\n\n::: {.cell-output-display}\n![Training and testing datapoints plotted.](regression_files/figure-html/plot-data-1.png){width=672}\n:::\n:::\n\n1. Load the `ggplot2` and `patchwork` libraries. We'll be using these to produce plots and display them. If they're not found or not installed, you can install them using the standard install command: `install.packages(\"ggplot2\")` (and again for `patchwork`).\n2. Create a ggplot object for the training data.\n3. Parameter `data` specifies the dataframe to use for the plot. We're using the training data dataframe.\n4. Parameter `mapping` specifies the mapping of the dataframe columns to the plot axes, i.e. what our x variable and y variable are in terms of column names. These go in the aes function, for specifying aesthetics of the plot.\n5. Specify the plot type; `geom_point` is a basic scatter plot, ideal for plotting individual data points. Using `geom_scatter` could also work here.\n6. Set the x and y axis limits. This is optional, but I'm specifying them to manually the plots limits rather than auto-detecting.\n7. Set the plot title.\n8. Set the plot theme.\n9. Repeat steps 2-8 identically, but this time for the testing data.\n10. Combine (and show) the two plots into a single plot.\n\nWe can see that the data follows some sort of trend, but not one we can clearly identify yet. We'll start by trying to fit a linear regression line to the data to see if we can get a better idea of what the trend is.\n\n# Linear Regression\n\nThe intiutive start to investigating the regression fit of the data would be to start with the first order, or linear, regression. This is a simple model that assumes the relationship between x and y is linear. We will continue to use our variables $x$ and $y$ to represent the input and output of our model, respectively.\n\nWhen formulating our problem function, we want to find a function $h$ such that $h(x) \\approx y$. For every input $x$, we want the output $h(x)$ to be as close to the actual output $y$ as possible. The $h$ stands for *hypothesis*, and is in terms of $x$. The dimensions of x corresponds to how many features we have, and the dimension of y corresponds to how many labels we have. In this example, both dimensions are 1, since we only have one feature and one label.\n\nConsider the following model:\n$$h_w(x) = w^\\top x + b$$\n\nwhere $w$ is a vector of *weights* of size $n$ where $n$ is equvialent to the length of vector $x$; and $b$ is a *bias* term, a vector on constants the same dimension as $x$. The bias term is a constant value that is added to the output of the model, the same concept you know from a basic linear function $y = mx + b$. The weights are multiplied by the input features, and then added to the bias term to get the output of the model.\n\nSince having the bias term adds a level of complexity to the formula, we can adjust the weights to include the bias term by adding a column of 1’s to the input features, which would have the same effect. The simplified notation would then be:\n\n$$h_w(x) = w^\\top x$$\n\nwhere $w$ is a weight vector that includes the bias term, i.e. both $w$ and $x$ are now of size $n+1$. That extra column included in $w$ is a column of 1’s, and the extra column included in $x$ is the vector $b$. In essence, we hide the existence of the bias term inside the weights vector; one less variable for us to worry about.\n\nThe above formula would be equivalent to the following:\n$$\nh_w(x) =\n\\begin{bmatrix}\n    w_1 \\\\\n    w_2 \\\\\n    ... \\\\\n    w_n\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n    x_1 & x_2 & ... & x_n\n\\end{bmatrix}\n$$\n\n## Model Solving\nWe'll work with the training data only for now, as once we have a weights vector and a completed model, we can then use the testing data to evaluate the model. To relate our data to the linear regression model, we first have to add the column vector of 1’s to the features to account for the bias term. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Add column of 1's to training features\nx_tr_1 <- cbind(                # <1>\n    x_tr,                       # <2>\n    b = rep(1, length(x_tr))    # <3>\n)                               # <1>\n```\n:::\n\n1. Create a new variable, `x_tr_1`, that is the result of the `cbind` function. This function takes vectors (or dataframes) and combines them by their columns, hence the name `cbind` for *column bind*. We will want to combine our original features with a column of 1's of the same length. \n2. Specify our first vector to combine, `x_tr`. This is the original training features vector.\n3. The second vector to combine will be a column containing just ones of the same length as our orignal vector. We start with `b =` to name our column consistently with the formula. We can create this column of 1's on the fly (instead of manually) by using the `rep` function. The `rep` function takes a value and repeats it a specified number of times. In this case, we want to repeat the value 1, `length(x_tr)` times. The `length` function returns the length of a vector, in this case the number of elements in `x_tr`.\n\nNow `x_tr_1` looks like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(x_tr_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x_tr b\n1 3.1381904 1\n2 3.4421490 1\n3 1.3083408 1\n4 0.9954419 1\n5 3.2414549 1\n6 2.8537392 1\n```\n\n\n:::\n:::\n\n\n:::{.callout-note appearance=\"simple\"}\nLike with our `_tr` suffix, I use the `_1` suffix to represent variables for linear regression. Later on I'll be using other regression models such as `_2` for second-order polynomial regression, `_3` for third-order, etc.\n:::\n\nNow that we have our training features with the bias term included, we can use the training labels to solve for the weights vector by rearranging the linear regression formula. Using matrix algebra, we derive the formula for solving for the weights vector:\n\n$$w = (X^\\top X)^{-1} X^\\top y$$\n\nwhere $X$ is the matrix form of training features, $y$ is the vector of training labels, and $w$ is the vector of weights. The $X^\\top$ notation is the transpose of $X$, and the $X^{-1}$ notation is the inverse of $X$. The inverse of a matrix is a matrix that when multiplied by the original matrix, results in the identity matrix. The identity matrix is a square matrix with 1’s on the diagonal and 0’s everywhere else. The identity matrix is the multiplicative identity, meaning that when multiplied by any matrix, the result is the original matrix. The inverse of a matrix is the multiplicative inverse, meaning that when multiplied by the original matrix, the result is the identity matrix.\n\nMatrix operations are available in base R, including `%*%` for matrix multiplication, `t` for transposing a matrix, and `solve` for solving a matrix equation. When `solve` is only provided one parameter, it solves for the inverse of the input matrix. We can use these functions to solve for the weights vector.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert training dataframes to matrices\ny_tr_mat <- as.matrix(y_tr)     # <1>\nx_tr_1 <- as.matrix(x_tr_1)     # <1>\n\n# Solve for weight vector using training data\nw <- solve(t(x_tr_1) %*% x_tr_1) %*% (t(x_tr_1) %*% y_tr_mat)   # <2>\n```\n:::\n\n1. To perform matrix operations on our dataframes, we need to first convert them to matrices. We can do this using the `as.matrix` function. I rename `y_tr` to `y_tr_mat` to store its matrix variable separately, since it will be reused later on in the code.\n2. Solve for `w` using the formula above, with `x_tr_1` as X and `y_tr_mat` as y. We use the `%*%` operator for matrix multiplication, the `solve` function to solve for the inverse of a matrix, and the `t` function to transpose a matrix. The brackets are there to visually support the order of operations.\n\nNow that we have our weights vector $w$, the full linear regression formula is complete and we can plot the resulting function on top of our training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot linear regression on training set\nplt_tr_1 <- ggplot(                     # <1>\n    data = df_tr,                       # <2>\n    mapping = aes(x = x_tr, y = y_tr)   # <3>\n) +                                     # <1>\n    geom_point() +                      # <4>\n    stat_function(fun = function(x) {   # <5>\n        w[1] * x + w[2]                 # <5>\n    }) +                                # <5>\n    xlim(0, 4) +                        # <6>\n    ylim(0, 30) +                       # <6>\n    theme_light()                       # <7>\n\nplt_tr_1                                # <8>\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/plot-linreg-1.png){width=672}\n:::\n:::\n\n1. Create a ggplot object for the linear regression over the training data.\n2. Parameter `data` specifies the dataframe to use for the plot. We're using the training data dataframe.\n3. Parameter `mapping` specifies the mapping of the dataframe columns to the plot axes, i.e. what our x variable and y variable are in terms of column names. These go in the aes function, for specifying aesthetics of the plot.\n4. Specify the plot type; `geom_point` is a basic scatter plot, ideal for plotting individual data points. Using `geom_scatter` could also work here.\n5. Specify another plot type (alongside `geom_point`). Additional plot types can be added to render on top of the main data (the training data). Here we are plotting a function using `stat_function` by passing in the function as paramter `fun`. The function is created in `y = mx + b` format from our weights vector `w`, so function of x is `w[1] * x + w[2]` where `w[1]` is the slope `m` and `w[2]` is the y-intercept `b`. The function is plotted over the range of the training data, which is why we don't need to specify `x` in the `stat_function` function.\n6. Set the x and y axis limits. This is optional, but I'm specifying them to manually the plots limits rather than auto-detecting.\n7. Set the plot theme.\n8. Display the plot.\n\n## Average Error\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute average training error\neq_tr_1 <- t(as.data.frame(t(w) %*% t(x_tr_1)))\nerrors_tr_1 <- (eq_tr_1 - y_tr_mat)**2\nerr_avg_tr_1 <- colSums(errors_tr_1) / length(errors_tr_1)\nerr_avg_tr_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    y_tr \n2.143112 \n```\n\n\n:::\n:::\n\nThe average error of the training set using the linear regression model is 2.287073.\n\nPlot both the regression line and the test data on the same graph. Also report the average error on the test set using Eq. (1).\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert training outputs to matrix\ny_te_mat <- as.matrix(y_te)\n\n# add column of 1s to testing features\nx_te_1 <- cbind(x_te, b = rep(1, length(x_te)))\nx_te_1 <- as.matrix(x_te_1)\n\n# plot linear regression on test set\nplt_te_1 <- ggplot(\n    data = df_te,\n    mapping = aes(x = x_te, y = y_te)\n) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        w[1] * x + w[2]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_1\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/plot-linreg-train-error-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average test error using same weight vector from training data\neq_te_1 <- t(as.data.frame(t(w) %*% t(x_te_1)))\nerrors_te_1 <- (eq_te_1 - y_te_mat)**2\nerr_avg_te_1 <- colSums(errors_te_1) / length(errors_te_1)\nerr_avg_te_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    y_tr \n9.973294 \n```\n\n\n:::\n\n```{.r .cell-code}\n# store average errors:\nerrs <- data.frame(\n    \"training\" = err_avg_tr_1,\n    \"testing\" = err_avg_te_1,\n    row.names = \"order1\"\n)\n```\n:::\n\nThe average error of the testing set using the linear regression model is 9.427358.\n\n# Polynomial Regression\n\n## Second Order Polynomial\n\nImplement the 2nd-order polynomial regression by adding new features x2 to the inputs. Repeat (b) and (c). Compare the training error and test error. Is it a better fit than linear regression?\n\n::: {.cell}\n\n```{.r .cell-code}\n# add second order polynomial to features\nx_tr_2 <- as.matrix(x_tr_1[, 1]**2)\nx_tr_2 <- cbind(x_tr_2, x_tr_1)\n\n# compute weight vector\nw2 <- solve(t(x_tr_2) %*% x_tr_2) %*% (t(x_tr_2) %*% y_tr_mat)\n\n# plot polynomial regression on training set\nplt_tr_2 <- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w2[1] * x^2) + (w2[2] * x) + w2[3]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_2\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average training error\neq_tr_2 <- t(as.data.frame(t(w2) %*% t(x_tr_2)))\nerrors_tr_2 <- (eq_tr_2 - y_tr_mat)**2\nerr_avg_tr_2 <- colSums(errors_tr_2) / length(errors_tr_2)\nerr_avg_tr_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   y_tr \n1.98476 \n```\n\n\n:::\n\n```{.r .cell-code}\n# add columns to testing features\nx_te_2 <- as.matrix(x_te_1[, 1]**2)\nx_te_2 <- cbind(x_te_2, x_te_1)\n\n# plot polynomial regression on test set\nplt_te_2 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w2[1] * x^2) + (w2[2] * x) + w2[3]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_2\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average testing error using same weight vector from training data\neq_te_2 <- t(as.data.frame(t(w2) %*% t(x_te_2)))\nerrors_te_2 <- (eq_te_2 - y_te_mat)**2\nerr_avg_te_2 <- colSums(errors_te_2) / length(errors_te_2)\nerr_avg_te_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    y_tr \n12.16348 \n```\n\n\n:::\n\n```{.r .cell-code}\n# store average errors\nerrs <- rbind(errs, order2 = c(err_avg_tr_2, err_avg_te_2))\nerrs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       training   testing\norder1 2.143112  9.973294\norder2 1.984760 12.163477\n```\n\n\n:::\n:::\n\nComparing the training error and test error, this second order polynomial regression model performs worse than the linear model, with a better training fit but a worse testing fit. Thus, the linear regression model is a better fit.\n\n## Third Order Polynomial\n\nImplement the 3rd-order polynomial regression by adding new features x2,x3 to the inputs. Repeat (b) and (c). Compare the training error and test error. Is it a better fit than linear regression and 2nd-order polynomial regression?\n\n::: {.cell}\n\n```{.r .cell-code}\n# add third order polynomial to features\nx_tr_3 <- as.matrix(x_tr_1[, 1]**3)\nx_tr_3 <- cbind(x_tr_3, x_tr_2)\n\n# compute weight vector\nw3 <- solve(t(x_tr_3) %*% x_tr_3) %*% (t(x_tr_3) %*% y_tr_mat)\n\n# plot polynomial regression on training set\nplt_tr_3 <- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w3[1] * x^3) + (w3[2] * x^2) + (w3[3] * x) + w3[4]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_3\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average training error\neq_tr_3 <- t(as.data.frame(t(w3) %*% t(x_tr_3)))\nerrors_tr_3 <- (eq_tr_3 - y_tr_mat)**2\nerr_avg_tr_3 <- colSums(errors_tr_3) / length(errors_tr_3)\nerr_avg_tr_3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     y_tr \n0.8850264 \n```\n\n\n:::\n\n```{.r .cell-code}\n# add columns to testing features\nx_te_3 <- as.matrix(x_te_1[, 1]**3)\nx_te_3 <- cbind(x_te_3, x_te_2)\n\n# plot polynomial regression on test set\nplt_te_3 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w3[1] * x^3) + (w3[2] * x^2) + (w3[3] * x) + w3[4]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_3\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average testing error using same weight vector from training data\neq_te_3 <- t(as.data.frame(t(w3) %*% t(x_te_3)))\nerrors_te_3 <- (eq_te_3 - y_te_mat)**2\nerr_avg_te_3 <- colSums(errors_te_3) / length(errors_te_3)\nerr_avg_te_3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    y_tr \n2.715822 \n```\n\n\n:::\n\n```{.r .cell-code}\n# store average errors\nerrs <- rbind(errs, order3 = c(err_avg_tr_3, err_avg_te_3))\nerrs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        training   testing\norder1 2.1431122  9.973294\norder2 1.9847596 12.163477\norder3 0.8850264  2.715822\n```\n\n\n:::\n:::\n\nThis time, the training error has improved much more than with the second order model. In addition, the testing error has also dramatically improved, even better than our original best linear regression model. Overall, this third order polynomial regression is a better fit than both the linear and second order models.\n\n## Fourth Order Polynomial\nImplement the 4th-order polynomial regression by adding new features x2,x3,x4 to the inputs. Repeat (b) and (c). Compare the training error and test error. Compared with the previous results, which order is the best for fitting the data?\n\n::: {.cell}\n\n```{.r .cell-code}\n# add fourth order polynomial to features\nx_tr_4 <- as.matrix(x_tr_1[, 1]**4)\nx_tr_4 <- cbind(x_tr_4, x_tr_3)\n\n# compute weight vector\nw4 <- solve(t(x_tr_4) %*% x_tr_4) %*% (t(x_tr_4) %*% y_tr_mat)\neq_tr_4 <- t(as.data.frame(t(w4) %*% t(x_tr_4)))\n\n# plot polynomial regression on training set\nplt_tr_4 <- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_4\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average training error\nerrors_tr_4 <- (eq_tr_4 - y_tr_mat)**2\nerr_avg_tr_4 <- colSums(errors_tr_4) / length(errors_tr_4)\nerr_avg_tr_4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     y_tr \n0.8833612 \n```\n\n\n:::\n\n```{.r .cell-code}\n# add columns to testing features\nx_te_4 <- as.matrix(x_te_1[, 1]**4)\nx_te_4 <- cbind(x_te_4, x_te_3)\n\n# equation using same weight vector from training data\neq_te_4 <- t(as.data.frame(t(w4) %*% t(x_te_4)))\n\n# plot polynomial regression on test set\nplt_te_4 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_4\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average testing error using same weight vector from training data\nerrors_te_4 <- (eq_te_4 - y_te_mat)**2\nerr_avg_te_4 <- colSums(errors_te_4) / length(errors_te_4)\nerr_avg_te_4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   y_tr \n3.05486 \n```\n\n\n:::\n\n```{.r .cell-code}\n# store average errors\nerrs <- rbind(errs, order4 = c(err_avg_tr_4, err_avg_te_4))\nerrs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        training   testing\norder1 2.1431122  9.973294\norder2 1.9847596 12.163477\norder3 0.8850264  2.715822\norder4 0.8833612  3.054860\n```\n\n\n:::\n:::\n\nUsing fourth order polynomial regression, our training error has slightly improved, however, the testing error has slightly gotten worse, by a larger amount. While it is close to the third order polynomial regression, overall the data has become slightly overfitted, and so the best model for fitting our data is the third order polynomial regression model.\n\n# Regularization and Cross-Validation\n\nUsing the training data to implement l2-regularized for the 4th-order polynomial regression (page 12 of Lecture 4, note that we do not penalize the bias term w0), vary the regularization parameter λ ∈ {0.01, 0.05, 0.1, 0.5, 1, 100, 106 }. Plot the training and test error (averaged over all instances) using Eq. (1) as a function of λ (you should use a log10 scale for λ). Which λ is the best for fitting the training data? Which λ is the best for fitting the test data?\n\n::: {.cell}\n\n```{.r .cell-code}\n# create identity matrix with 0 at position (1, 1)\ni_hat <- diag(nrow(w4))\ni_hat[1, 1] <- 0\n\n# function calculating training error in terms of lambda\nl2_error_tr <- function(lambda) {\n    l <- 0.5 * norm(x_tr_4 %*% w4 - y_tr_mat)^2 +\n        lambda / 2 * t(w4) %*% i_hat %*% w4\n    return(l)\n}\n# function calculating testing error in terms of lambda\nl2_error_te <- function(lambda) {\n    l <- 0.5 * norm(x_te_4 %*% w4 - y_te_mat)^2 +\n        lambda / 2 * t(w4) %*% i_hat %*% w4\n    return(l)\n}\n\n# calculate values based on given lambdas\nlambdas <- c(0.01, 0.05, 0.1, 0.5, 1, 100, 10^6)\nl2_tr <- sapply(lambdas, l2_error_tr)\nl2_te <- sapply(lambdas, l2_error_te)\n\n# save to dataframe\ndf_l2 <- data.frame(lambdas, l2_tr, l2_te)\ndf_l2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lambdas        l2_tr        l2_te\n1   1e-02 4.247305e+02 3.131591e+02\n2   5e-02 4.300739e+02 3.185025e+02\n3   1e-01 4.367531e+02 3.251818e+02\n4   5e-01 4.901873e+02 3.786159e+02\n5   1e+00 5.569799e+02 4.454086e+02\n6   1e+02 1.378193e+04 1.367036e+04\n7   1e+06 1.335857e+08 1.335856e+08\n```\n\n\n:::\n\n```{.r .cell-code}\n# plot data\nplt_reg <- ggplot() +\n    geom_point(\n        data = df_l2,\n        mapping = aes(x = lambdas, y = l2_tr, color = \"training\")\n    ) +\n    stat_function(fun = l2_error_tr, mapping = aes(color = \"training\")) +\n    geom_point(\n        data = df_l2,\n        mapping = aes(x = lambdas, y = l2_te, color = \"testing\"),\n    ) +\n    stat_function(fun = l2_error_te, mapping = aes(color = \"testing\")) +\n    scale_color_manual(\n        name = \"Dataset\",\n        values = c(\n            training = \"red\",\n            testing = \"blue\"\n        ),\n    ) +\n    scale_x_log10() +\n    scale_y_log10() +\n    xlab(\"log10 lambda\") +\n    ylab(\"log10 error\") +\n    theme_light()\nplt_reg\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in lambda/2 * t(w4) %*% i_hat %*% w4: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\nWarning in lambda/2 * t(w4) %*% i_hat %*% w4: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nThe smallest lambda value, 0.01, seems to minimize the l2-normalized loss for both the training and testing data with the minimized lambda value. At smaller lambda values, the testing error is greater than the training data, but as they get larger this difference is less visible.\n\nPlot the value of each weight parameter (including the bias term w0) as a function of λ.\n\n::: {.cell}\n\n```{.r .cell-code}\n# function of training weight in terms of lambda\nl2_w_tr <- function(lambda) {\n    w <- solve(t(x_tr_4) %*% x_tr_4 + lambda * i_hat) %*% t(x_tr_4) %*% y_tr_mat\n    return(w)\n}\n# compute weights based on lambdas\nw_tr <- sapply(lambdas, l2_w_tr)\n\n# create dataframe to plot data\ncols <- c(\"lambdas\", \"x4.term\", \"x3.term\", \"x2.term\", \"x.term\", \"bias.term\")\ndf_w_tr <- data.frame(lambdas, t(w_tr))\ncolnames(df_w_tr) <- cols\n\n# plot training weights\nplt_w_tr <- ggplot() +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = x4.term, color = \"x4\")\n    ) +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = x3.term, color = \"x3\")\n    ) +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = x2.term, color = \"x2\")\n    ) +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = x.term, color = \"x\")\n    ) +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = bias.term, color = \"bias\")\n    ) +\n    scale_color_manual(\n        name = \"weight parameter\",\n        values = c(\n            x4 = \"red\",\n            x3 = \"orange\",\n            x2 = \"purple\",\n            x = \"darkgreen\",\n            bias = \"blue\"\n        ),\n    ) +\n    scale_x_log10() +\n    xlab(\"log10 lambda\") +\n    ylab(\"weight parameter value\") +\n    theme_light()\nplt_w_tr\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# function of testing weight in terms of lambda\nl2_w_te <- function(lambda) {\n    w <- solve(t(x_te_4) %*% x_te_4 + lambda * i_hat) %*% t(x_te_4) %*% y_te_mat\n    return(w)\n}\n\n# compute testing weights\nw_te <- sapply(lambdas, l2_w_te)\n\n# create dataframe to plot data\ndf_w_te <- data.frame(lambdas, t(w_te))\ncolnames(df_w_te) <- cols\n\n# plot testing weights in terms of lambda\nplt_w_te <- ggplot() +\n    # x4.term\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = x4.term, color = \"x4\")\n    ) +\n    # x3.term\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = x3.term, color = \"x3\")\n    ) +\n    # x2.term\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = x2.term, color = \"x2\")\n    ) +\n    # x.term\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = x.term, color = \"x\")\n    ) +\n    # bias.term\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = bias.term, color = \"bias\")\n    ) +\n    scale_color_manual(\n        name = \"weight parameter\",\n        values = c(\n            x4 = \"red\",\n            x3 = \"orange\",\n            x2 = \"purple\",\n            x = \"darkgreen\",\n            bias = \"blue\"\n        ),\n    ) +\n    scale_x_log10() +\n    xlab(\"log10 lambda\") +\n    ylab(\"weight parameter value\") +\n    theme_light()\nplt_w_te\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\nNote: Data is plotted as discrete based on specifed lambda values.\n\nC\nWrite a procedure that performs five-fold cross-validation on your training data (page 7 of Lecture 4). Use it to determine the best value for λ. Show the average error on the validation set as a function of λ. Is the same as the best λ in (a)? For the best fit, plot the test data and the l2-regularized 4th-order polynomial regression line obtained.\n\n::: {.cell}\n\n```{.r .cell-code}\n# store training data into single dataframe\ntrain <- data.frame(x_tr_4, y_tr_mat)\n# shuffle the rows randomly\ntrain <- train[sample(nrow(train)), ]\n# split the data into 5 equal groups of 8, in a list\ngroups <- cut(seq_len(nrow(train)), breaks = 5, labels = FALSE)\ntrain_groups <- split(train, groups)\ntrain_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`1`\n             V1           V2          V3      x_tr b      y_tr\n11 1.318950e+01  6.921039493  3.63173556 1.9057113 1 10.604819\n12 1.062850e-02  0.033101986  0.10309463 0.3210835 1  5.425063\n25 6.532219e+00  4.085973504  2.55582059 1.5986934 1 10.889961\n17 4.591374e-03  0.017638311  0.06775968 0.2603069 1  3.994465\n20 1.937759e-04  0.001642384  0.01392034 0.1179845 1  2.794276\n3  2.930107e+00  2.239559755  1.71175565 1.3083408 1  9.349968\n26 1.415590e+02 41.039591401 11.89785537 3.4493268 1 18.209443\n32 3.698900e-04  0.002667193  0.01923253 0.1386814 1  3.135994\n\n$`2`\n            V1         V2         V3      x_tr b      y_tr\n5  110.3976767 34.0580635 10.5070299 3.2414549 1 14.950901\n36   0.1368850  0.2250437  0.3699797 0.6082596 1  7.236291\n34  20.4583837  9.6195220  4.5230945 2.1267568 1  9.804237\n13   0.1254027  0.2107318  0.3541224 0.5950819 1  7.904996\n14   2.8219546  2.1772711  1.6798674 1.2960970 1  8.971721\n24   3.4800558  2.5479427  1.8654908 1.3658297 1 10.988057\n39  20.8853169  9.7696904  4.5700456 2.1377665 1 13.405720\n23 111.2239867 34.2490747 10.5462783 3.2475034 1 16.648354\n\n$`3`\n             V1         V2         V3      x_tr b     y_tr\n6   66.32192507 23.2403596  8.1438274 2.8537392 1 12.89095\n22  98.37402895 31.2363546  9.9183683 3.1493441 1 16.56782\n9    0.04159386  0.0921026  0.2039457 0.4516035 1  7.32844\n1   96.98781114 30.9056490  9.8482390 3.1381904 1 15.28901\n19 107.43941772 33.3712700 10.3652987 3.2195184 1 13.29684\n18   8.26173568  4.8730792  2.8743235 1.6953830 1 12.29269\n4    0.98189188  0.9863879  0.9909046 0.9954419 1 10.10585\n\n$`4`\n             V1           V2           V3       x_tr b      y_tr\n21 1.956219e+01 9.301718e+00  4.422916885 2.10307320 1 13.165072\n28 2.086426e-01 3.087111e-01  0.456774142 0.67585068 1  9.142000\n37 5.519668e-03 2.025045e-02  0.074294465 0.27257011 1  5.415621\n10 1.466429e+02 4.214012e+01 12.109620493 3.47988800 1 17.327588\n33 6.213827e+01 2.213194e+01  7.882783409 2.80762950 1 14.299533\n29 5.544779e+00 3.613376e+00  2.354735365 1.53451470 1 11.422151\n16 6.761235e+01 2.357868e+01  8.222672671 2.86752030 1 14.140329\n15 5.337705e-07 1.974767e-05  0.000730596 0.02702954 1  1.609707\n\n$`5`\n             V1           V2           V3        x_tr b      y_tr\n31 7.417691e-02 1.421352e-01 2.723544e-01 0.521875830 1  7.173304\n27 5.325306e-09 6.233880e-07 7.297469e-05 0.008542523 1  2.277592\n8  6.523413e+01 2.295388e+01 8.076765e+00 2.841965000 1 13.414665\n7  4.896759e+01 1.851107e+01 6.997685e+00 2.645313700 1 11.362217\n38 1.453461e+01 7.443935e+00 3.812428e+00 1.952544100 1 11.138174\n35 8.032714e+01 2.683161e+01 8.962541e+00 2.993750300 1 14.940644\n2  1.403843e+02 4.078392e+01 1.184839e+01 3.442149000 1 19.023864\n30 3.316681e+01 1.382063e+01 5.759064e+00 2.399804900 1 11.915797\n```\n\n\n:::\n\n```{.r .cell-code}\n# loop for each run\nfor (run in 1:5) {\n    # designate datasets\n    validation_set <- train_groups[[run]]\n    training_set <- do.call(rbind, train_groups[-run])\n    x <- as.matrix(training_set[1:5])\n    y <- as.matrix(training_set[6])\n\n    # compute weight vector\n    w <- solve(t(x) %*% x) %*% (t(x) %*% y)\n\n    # create identity matrix with 0 at position (1, 1)\n    i_hat <- diag(nrow(w))\n    i_hat[1, 1] <- 0\n\n    # function calculating validation error in terms of lambda\n    l2_error <- function(lambda) {\n        l <- 0.5 * norm(\n            as.matrix(validation_set[1:5]) %*% w - as.matrix(validation_set[6])\n        )^2 + lambda / 2 * t(w) %*% i_hat %*% w\n        return(l)\n    }\n\n    # plot data\n    plt <- ggplot() +\n        stat_function(fun = l2_error) +\n        scale_x_log10() +\n        labs(\n            title = paste0(\"Run \", run),\n            x = \"log10 lambda\",\n            y = \"error\",\n        ) +\n        ylim(0, 2000) +\n        theme_light()\n    print(plt)\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-6-4.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-6-5.png){width=672}\n:::\n:::\n\nWe can see slight variation in each figure, but the trend remains the same: the error has a positive correlation with lambda, thus minimizing lambda seems to be the best fit for our data. This is the same trend seen previously.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot 4th order polynomial regression on test set\nplt_te_4 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_4\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\nThe best fit 4th order polynomial regression plot.",
    "supporting": [
      "regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}