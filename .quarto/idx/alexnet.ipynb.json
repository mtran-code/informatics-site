{"title":"Assignment 4","markdown":{"headingText":"Assignment 4","containsRefs":false,"markdown":"\n\nCS4442 Aritificial Intelligence II\n\nMichael Tran\n\n## Step 1\nUse one of the deep learning frameworks (Pytorch, Tensorflow, Keras, ...) and load the pre-trained AlexNet model. Input these 156 images to the pre-trained AlexNet model and extract feature maps/activations from Conv 1, 2, 3, 4, 5, fc6, and fc7 layers. Vectorize the activations corresponding to each image. You should have a vector of activations per image per layer mentioned above. (20 points)\n\n## Step 2\nCreate a representational dissimilarity matrix (RDM) which is 156 x156 matrix, each row and column in this matrix is indexed by one of the images in the image set and each element in the matrix is the Euclidean distance between the activation vectors of the corresponding images you extracted in Step 1. (30 points)\n\n## Step 3\nPlot the RDM for each layer (Conv 1, 2, 3, 4, 5, fc6, fc7), and their corresponding multidimensional scaling (MDS) visualization in 2 Dimension. The class labels you should use for the MDS plotting includes images 1 to 28 are Animals, 29-64 are Objects, 65 to 100 are scenes, 101 to 124 are human activities, 125 to 156 are faces. (30 points)\n\n### Please write a short report presenting and discussing the results of this assignment. (20 points)\n\nIntroduction\n\nIn this assignment, I inputted 156 different images of JPEG format into the pre-trained AlexNet model in order to map and visualize the image recognition of the labels (Animal, Object, Scene, Activity, Face). The AlexNet model was developed by Krizhevsky et al in 2012 as part of the ImageNet challenge to implement object recognition models. It is a convolutional neural network (CNN) consisting of five convolutional layers as well as additional fully connected layers for classification. The input dataset consists of 156 JPEG images of varying dimensions, in order of their respective class labels: 28 Animal, 36 Object, 36 Scene, 24 Activity, and 32 Face.\n\nMethods\n\nTo access the pre-trained AlexNet model and work with a deep learning models, I used the `PyTorch` deep learning framework, an open source library available in Python developed by Facebook's AI research team. PyTorch has pre-trained models available and accessible within the package, including AlexNet pre-trained with weights. I imported this and also prepared a transformer to take in the varying JPEG inputs and transform them to the standard ImageNet specifications. This involves resizing them to the correct image dimensions taken by the model, 227 x 227 pixels, and normalizing the pixel values to the mean and standard deviation of the ImageNet dataset. Next I looped over each of my input images, loaded them into Python using the `PIL` package, and applied my transformer to each of them. Now that I had my batch of images, I could feed them to the model.\n\nFrom the results I extracted feature maps / activations from each of the layers Conv1, Conv2, Conv3, Conv4, Conv5, fc6, and fc7. I stored these activations in a dictionary named `activations` and vectorized each of them. I also created a representational dissimilarity matrix (RDM) named `RDMs` from the vectors that represents the Euclidian distances between them.\n\nResults\n\nUsing the `matplotlib` library in Python, I plotted my results from the model, beginning with the RDM matrix. I generated RDM plots of each of the layers mentioned previously. This gave me seven RDM plots, representing the similarity betwwen the images in each layer of the CNN. Using the colour scale, we can observe pairwise similarities between the inputs. Along the diagonal is a noticable black line, and this is because when comparing an image to itself, it is expected that there is zero dissimilarity. Using the colour scale, I can observe a high level of dissimilarity between images as the plots seem to be generally light in colour, which corresponds to a high disimilarity. There is a noticeable exception to this in the bottom right corner, where the last subset of images show a darker square, representing a closer similarity to each other. The last subset of images in the input dataset represents the group Faces, so it is interesting to observe that these images are relatively closer to one another than the rest of the images. Otherwise, we can also see slight clustering of class labels through colour differences, creating a faint checkered-like pattern. Each of the squares would represent a class label, as naturally images belonging to the same group are expected to have a similar level of closeness to one another than with other groups. As we plot more layers, the RDM plots show overall less dissimilarity.\n\nNext, I plotted the results of the model using Multidimenional Scaling (MDS) in order to visualize and compare the similarity between images of the CNN. By plotting for each layer, I could explore how the MDS representations change as they are processed through each layer of the CNN. The overall trend seen through the resulting plots is that each layer had distict patterns, with the first layers likely grouping based on low-level features whilst the latter layers group based on high-level semantic features corresponding to the categories, i.e. Animals, Scenes, Faces, etc. Looking at the colour legend, we can see that the clustering of group improves with each layer, where images of the same class label are closer to one another. In particular, the cluster corresponding to the Face label are very tightly clustered even from the first layer. This complements our findings from the RDM plots, where we noticed that the Face images have a relatively higher similarity to one another compared to the rest of the images. We can even see this cluster form a slight gap between itself and the rest of the images in the MDS plots, highlighting their difference as a group.\n\nConclusion\n\nOverall, the results of the assignment provide insight on how images are processed in a convolutional neural network, namely AlexNet. By visualizing the similarity between images in different layers, we gain a better understanding of how the network classifies and recognizes different groups such as objects, scenes, or faces. We also explore how different class labels of images compare to one another in similarity and image recognition efficiency.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"alexnet.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}}}