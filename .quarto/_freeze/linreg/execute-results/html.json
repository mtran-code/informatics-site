{
  "hash": "ef8ba4b4ee9d002404b00d02fabc7385",
  "result": {
    "markdown": "---\ntitle: \"Linear Regression in R\"\n---\n\n\n# Linear and Polynomial Regression\n\nA\n\n::: {.cell}\n\n```{.r .cell-code}\n# setup libraries\nlibrary(ggplot2)\n\n# read in first dataset into dataframe\nx_tr <- read.delim(file = \"data/hw1xtr.dat\", header = FALSE, col.names = \"x_tr\")\ny_tr <- read.delim(file = \"data/hw1ytr.dat\", header = FALSE, col.names = \"y_tr\")\ndf_tr <- data.frame(x_tr, y_tr)\n\n# read in second dataset into dataframe\nx_te <- read.delim(file = \"data/hw1xte.dat\", header = FALSE, col.names = \"x_te\")\ny_te <- read.delim(file = \"data/hw1yte.dat\", header = FALSE, col.names = \"y_te\")\ndf_te <- data.frame(x_te, y_te)\n\n# plot training dataset\nplt_tr <- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    labs(title = \"Training data\") +\n    theme_light()\nplt_tr\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# plot test dataset\nplt_te <- ggplot(df_te, aes(x = x_te, y = y_te)) +\n    geom_point() +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    labs(title = \"Testing data\") +\n    theme_light()\nplt_te\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\nB\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert training outputs to matrix\ny_tr_mat <- as.matrix(y_tr)\n\n# add column of 1s to training features\nx_tr_1 <- cbind(x_tr, b = rep(1, length(x_tr)))\nx_tr_1 <- as.matrix(x_tr_1)\n\n# solve for weight vector using training features\nw <- solve(t(x_tr_1) %*% x_tr_1) %*% (t(x_tr_1) %*% y_tr_mat)\n\n# plot linear regression on training set\nplt_tr_1 <- ggplot(data = df_tr, mapping = aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        w[1] * x + w[2]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_1\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average training error\neq_tr_1 <- t(as.data.frame(t(w) %*% t(x_tr_1)))\nerrors_tr_1 <- (eq_tr_1 - y_tr_mat)**2\nerr_avg_tr_1 <- colSums(errors_tr_1) / length(errors_tr_1)\nerr_avg_tr_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    y_tr \n2.287073 \n```\n:::\n:::\n\nThe average error of the training set using the linear regression model is 2.287073.\n\nC\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert training outputs to matrix\ny_te_mat <- as.matrix(y_te)\n\n# add column of 1s to testing features\nx_te_1 <- cbind(x_te, b = rep(1, length(x_te)))\nx_te_1 <- as.matrix(x_te_1)\n\n# plot linear regression on test set\nplt_te_1 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        w[1] * x + w[2]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_1\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average test error using same weight vector from training data\neq_te_1 <- t(as.data.frame(t(w) %*% t(x_te_1)))\nerrors_te_1 <- (eq_te_1 - y_te_mat)**2\nerr_avg_te_1 <- colSums(errors_te_1) / length(errors_te_1)\nerr_avg_te_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    y_tr \n9.427358 \n```\n:::\n\n```{.r .cell-code}\n# store average errors:\nerrs <- data.frame(\n    \"training\" = err_avg_tr_1,\n    \"testing\" = err_avg_te_1,\n    row.names = \"order1\"\n)\n```\n:::\n\nThe average error of the testing set using the linear regression model is 9.427358.\n\nD\n\n::: {.cell}\n\n```{.r .cell-code}\n# add second order polynomial to features\nx_tr_2 <- as.matrix(x_tr_1[, 1]**2)\nx_tr_2 <- cbind(x_tr_2, x_tr_1)\n\n# compute weight vector\nw2 <- solve(t(x_tr_2) %*% x_tr_2) %*% (t(x_tr_2) %*% y_tr_mat)\n\n# plot polynomial regression on training set\nplt_tr_2 <- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w2[1] * x^2) + (w2[2] * x) + w2[3]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_2\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average training error\neq_tr_2 <- t(as.data.frame(t(w2) %*% t(x_tr_2)))\nerrors_tr_2 <- (eq_tr_2 - y_tr_mat)**2\nerr_avg_tr_2 <- colSums(errors_tr_2) / length(errors_tr_2)\nerr_avg_tr_2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    y_tr \n2.105887 \n```\n:::\n\n```{.r .cell-code}\n# add columns to testing features\nx_te_2 <- as.matrix(x_te_1[, 1]**2)\nx_te_2 <- cbind(x_te_2, x_te_1)\n\n# plot polynomial regression on test set\nplt_te_2 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w2[1] * x^2) + (w2[2] * x) + w2[3]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_2\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average testing error using same weight vector from training data\neq_te_2 <- t(as.data.frame(t(w2) %*% t(x_te_2)))\nerrors_te_2 <- (eq_te_2 - y_te_mat)**2\nerr_avg_te_2 <- colSums(errors_te_2) / length(errors_te_2)\nerr_avg_te_2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    y_tr \n11.73039 \n```\n:::\n\n```{.r .cell-code}\n# store average errors\nerrs <- rbind(errs, order2 = c(err_avg_tr_2, err_avg_te_2))\nerrs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       training   testing\norder1 2.287073  9.427358\norder2 2.105887 11.730392\n```\n:::\n:::\n\nComparing the training error and test error, this second order polynomial regression model performs worse than the linear model, with a better training fit but a worse testing fit. Thus, the linear regression model is a better fit.\n\n\nE\n\n::: {.cell}\n\n```{.r .cell-code}\n# add third order polynomial to features\nx_tr_3 <- as.matrix(x_tr_1[, 1]**3)\nx_tr_3 <- cbind(x_tr_3, x_tr_2)\n\n# compute weight vector\nw3 <- solve(t(x_tr_3) %*% x_tr_3) %*% (t(x_tr_3) %*% y_tr_mat)\n\n# plot polynomial regression on training set\nplt_tr_3 <- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w3[1] * x^3) + (w3[2] * x^2) + (w3[3] * x) + w3[4]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_3\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average training error\neq_tr_3 <- t(as.data.frame(t(w3) %*% t(x_tr_3)))\nerrors_tr_3 <- (eq_tr_3 - y_tr_mat)**2\nerr_avg_tr_3 <- colSums(errors_tr_3) / length(errors_tr_3)\nerr_avg_tr_3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     y_tr \n0.8941722 \n```\n:::\n\n```{.r .cell-code}\n# add columns to testing features\nx_te_3 <- as.matrix(x_te_1[, 1]**3)\nx_te_3 <- cbind(x_te_3, x_te_2)\n\n# plot polynomial regression on test set\nplt_te_3 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w3[1] * x^3) + (w3[2] * x^2) + (w3[3] * x) + w3[4]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_3\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average testing error using same weight vector from training data\neq_te_3 <- t(as.data.frame(t(w3) %*% t(x_te_3)))\nerrors_te_3 <- (eq_te_3 - y_te_mat)**2\nerr_avg_te_3 <- colSums(errors_te_3) / length(errors_te_3)\nerr_avg_te_3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    y_tr \n2.483334 \n```\n:::\n\n```{.r .cell-code}\n# store average errors\nerrs <- rbind(errs, order3 = c(err_avg_tr_3, err_avg_te_3))\nerrs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        training   testing\norder1 2.2870726  9.427358\norder2 2.1058871 11.730392\norder3 0.8941722  2.483334\n```\n:::\n:::\n\nThis time, the training error has improved much more than with the second order model. In addition, the testing error has also dramatically improved, even better than our original best linear regression model. Overall, this third order polynomial regression is a better fit than both the linear and second order models.\n\nF\n\n::: {.cell}\n\n```{.r .cell-code}\n# add fourth order polynomial to features\nx_tr_4 <- as.matrix(x_tr_1[, 1]**4)\nx_tr_4 <- cbind(x_tr_4, x_tr_3)\n\n# compute weight vector\nw4 <- solve(t(x_tr_4) %*% x_tr_4) %*% (t(x_tr_4) %*% y_tr_mat)\neq_tr_4 <- t(as.data.frame(t(w4) %*% t(x_tr_4)))\n\n# plot polynomial regression on training set\nplt_tr_4 <- ggplot(df_tr, aes(x = x_tr, y = y_tr)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_tr_4\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average training error\nerrors_tr_4 <- (eq_tr_4 - y_tr_mat)**2\nerr_avg_tr_4 <- colSums(errors_tr_4) / length(errors_tr_4)\nerr_avg_tr_4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     y_tr \n0.8897772 \n```\n:::\n\n```{.r .cell-code}\n# add columns to testing features\nx_te_4 <- as.matrix(x_te_1[, 1]**4)\nx_te_4 <- cbind(x_te_4, x_te_3)\n\n# equation using same weight vector from training data\neq_te_4 <- t(as.data.frame(t(w4) %*% t(x_te_4)))\n\n# plot polynomial regression on test set\nplt_te_4 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_4\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute average testing error using same weight vector from training data\nerrors_te_4 <- (eq_te_4 - y_te_mat)**2\nerr_avg_te_4 <- colSums(errors_te_4) / length(errors_te_4)\nerr_avg_te_4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    y_tr \n3.011983 \n```\n:::\n\n```{.r .cell-code}\n# store average errors\nerrs <- rbind(errs, order4 = c(err_avg_tr_4, err_avg_te_4))\nerrs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        training   testing\norder1 2.2870726  9.427358\norder2 2.1058871 11.730392\norder3 0.8941722  2.483334\norder4 0.8897772  3.011983\n```\n:::\n:::\n\nUsing fourth order polynomial regression, our training error has slightly improved, however, the testing error has slightly gotten worse, by a larger amount. While it is close to the third order polynomial regression, overall the data has become slightly overfitted, and so the best model for fitting our data is the third order polynomial regression model.\n\n# Regularization and Cross-Validation\n\nA\n\n::: {.cell}\n\n```{.r .cell-code}\n# create identity matrix with 0 at position (1, 1)\ni_hat <- diag(nrow(w4))\ni_hat[1, 1] <- 0\n\n# function calculating training error in terms of lambda\nl2_error_tr <- function(lambda) {\n    l <- 0.5 * norm(x_tr_4 %*% w4 - y_tr_mat)^2 +\n        lambda / 2 * t(w4) %*% i_hat %*% w4\n    return(l)\n}\n# function calculating testing error in terms of lambda\nl2_error_te <- function(lambda) {\n    l <- 0.5 * norm(x_te_4 %*% w4 - y_te_mat)^2 +\n        lambda / 2 * t(w4) %*% i_hat %*% w4\n    return(l)\n}\n\n# calculate values based on given lambdas\nlambdas <- c(0.01, 0.05, 0.1, 0.5, 1, 100, 10^6)\nl2_tr <- sapply(lambdas, l2_error_tr)\nl2_te <- sapply(lambdas, l2_error_te)\n\n# save to dataframe\ndf_l2 <- data.frame(lambdas, l2_tr, l2_te)\ndf_l2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  lambdas        l2_tr        l2_te\n1   1e-02 4.492262e+02 3.395960e+02\n2   5e-02 4.552134e+02 3.455832e+02\n3   1e-01 4.626975e+02 3.530673e+02\n4   5e-01 5.225699e+02 4.129397e+02\n5   1e+00 5.974104e+02 4.877802e+02\n6   1e+02 1.541583e+04 1.530620e+04\n7   1e+06 1.496815e+08 1.496814e+08\n```\n:::\n\n```{.r .cell-code}\n# plot data\nplt_reg <- ggplot() +\n    geom_point(\n        data = df_l2,\n        mapping = aes(x = lambdas, y = l2_tr, color = \"training\")\n    ) +\n    stat_function(fun = l2_error_tr, mapping = aes(color = \"training\")) +\n    geom_point(\n        data = df_l2,\n        mapping = aes(x = lambdas, y = l2_te, color = \"testing\"),\n    ) +\n    stat_function(fun = l2_error_te, mapping = aes(color = \"testing\")) +\n    scale_color_manual(\n        name = \"Dataset\",\n        values = c(\n            training = \"red\",\n            testing = \"blue\"\n        ),\n    ) +\n    scale_x_log10() +\n    scale_y_log10() +\n    xlab(\"log10 lambda\") +\n    ylab(\"log10 error\") +\n    theme_light()\nplt_reg\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lambda/2 * t(w4) %*% i_hat %*% w4: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\nWarning in lambda/2 * t(w4) %*% i_hat %*% w4: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n:::\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\nThe smallest lambda value, 0.01, seems to minimize the l2-normalized loss for both the training and testing data with the minimized lambda value. At smaller lambda values, the testing error is greater than the training data, but as they get larger this difference is less visible.\n\nB\n\n::: {.cell}\n\n```{.r .cell-code}\n# function of training weight in terms of lambda\nl2_w_tr <- function(lambda) {\n    w <- solve(t(x_tr_4) %*% x_tr_4 + lambda * i_hat) %*% t(x_tr_4) %*% y_tr_mat\n    return(w)\n}\n# compute weights based on lambdas\nw_tr <- sapply(lambdas, l2_w_tr)\n\n# create dataframe to plot data\ncols <- c(\"lambdas\", \"x4.term\", \"x3.term\", \"x2.term\", \"x.term\", \"bias.term\")\ndf_w_tr <- data.frame(lambdas, t(w_tr))\ncolnames(df_w_tr) <- cols\n\n# plot training weights\nplt_w_tr <- ggplot() +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = x4.term, color = \"x4\")\n    ) +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = x3.term, color = \"x3\")\n    ) +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = x2.term, color = \"x2\")\n    ) +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = x.term, color = \"x\")\n    ) +\n    geom_line(\n        data = df_w_tr,\n        mapping = aes(x = lambdas, y = bias.term, color = \"bias\")\n    ) +\n    scale_color_manual(\n        name = \"weight parameter\",\n        values = c(\n            x4 = \"red\",\n            x3 = \"orange\",\n            x2 = \"purple\",\n            x = \"darkgreen\",\n            bias = \"blue\"\n        ),\n    ) +\n    scale_x_log10() +\n    xlab(\"log10 lambda\") +\n    ylab(\"weight parameter value\") +\n    theme_light()\nplt_w_tr\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# function of testing weight in terms of lambda\nl2_w_te <- function(lambda) {\n    w <- solve(t(x_te_4) %*% x_te_4 + lambda * i_hat) %*% t(x_te_4) %*% y_te_mat\n    return(w)\n}\n\n# compute testing weights\nw_te <- sapply(lambdas, l2_w_te)\n\n# create dataframe to plot data\ndf_w_te <- data.frame(lambdas, t(w_te))\ncolnames(df_w_te) <- cols\n\n# plot testing weights in terms of lambda\nplt_w_te <- ggplot() +\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = x4.term, color = \"x4\")\n    ) +\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = x3.term, color = \"x3\")\n    ) +\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = x2.term, color = \"x2\")\n    ) +\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = x.term, color = \"x\")\n    ) +\n    geom_line(\n        data = df_w_te,\n        mapping = aes(x = lambdas, y = bias.term, color = \"bias\")\n    ) +\n    scale_color_manual(\n        name = \"weight parameter\",\n        values = c(\n            x4 = \"red\",\n            x3 = \"orange\",\n            x2 = \"purple\",\n            x = \"darkgreen\",\n            bias = \"blue\"\n        ),\n    ) +\n    scale_x_log10() +\n    xlab(\"log10 lambda\") +\n    ylab(\"weight parameter value\") +\n    theme_light()\nplt_w_te\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\nNote: Data is plotted as discrete based on specifed lambda values.\n\nC\n\n::: {.cell}\n\n```{.r .cell-code}\n# store training data into single dataframe\ntrain <- data.frame(x_tr_4, y_tr_mat)\n# shuffle the rows randomly\ntrain <- train[sample(nrow(train)), ]\n# split the data into 5 equal groups of 8, in a list\ngroups <- cut(seq_len(nrow(train)), breaks = 5, labels = FALSE)\ntrain_groups <- split(train, groups)\ntrain_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`1`\n            V1         V2        V3      x_tr b      y_tr\n19  8.26173568  4.8730792 2.8743235 1.6953830 1 12.292687\n39 14.53461078  7.4439347 3.8124285 1.9525441 1 11.138174\n32  0.07417691  0.1421352 0.2723544 0.5218758 1  7.173304\n15  2.82195459  2.1772711 1.6798674 1.2960970 1  8.971721\n30  5.54477864  3.6133760 2.3547354 1.5345147 1 11.422151\n7  66.32192507 23.2403596 8.1438274 2.8537392 1 12.890953\n26  6.53221887  4.0859735 2.5558206 1.5986934 1 10.889961\n22 19.56219377  9.3017180 4.4229169 2.1030732 1 13.165072\n\n$`2`\n             V1           V2         V3      x_tr b      y_tr\n33  0.000369890  0.002667193 0.01923253 0.1386814 1  3.135994\n5   0.981891879  0.986387934 0.99090458 0.9954419 1 10.105849\n17 67.612345853 23.578680804 8.22267267 2.8675203 1 14.140329\n23 98.374028945 31.236354562 9.91836826 3.1493441 1 16.567816\n37  0.136884991  0.225043707 0.36997972 0.6082596 1  7.236291\n2  96.987811138 30.905649045 9.84823899 3.1381904 1 15.289011\n14  0.125402680  0.210731818 0.35412241 0.5950819 1  7.904996\n18  0.004591374  0.017638311 0.06775968 0.2603069 1  3.994465\n\n$`3`\n             V1           V2          V3      x_tr b      y_tr\n36 8.032714e+01 26.831609385  8.96254086 2.9937503 1 14.940644\n21 1.937759e-04  0.001642384  0.01392034 0.1179845 1  2.794276\n27 1.415590e+02 41.039591401 11.89785537 3.4493268 1 18.209443\n11 1.466429e+02 42.140123037 12.10962049 3.4798880 1 17.327588\n12 1.318950e+01  6.921039493  3.63173556 1.9057113 1 10.604819\n24 1.112240e+02 34.249074744 10.54627833 3.2475034 1 16.648354\n10 4.159386e-02  0.092102602  0.20394572 0.4516035 1  7.328440\n4  2.930107e+00  2.239559755  1.71175565 1.3083408 1  9.349968\n\n$`4`\n             V1           V2           V3        x_tr b      y_tr\n9  6.523413e+01 2.295388e+01 8.076765e+00 2.841965000 1 13.414665\n28 5.325306e-09 6.233880e-07 7.297469e-05 0.008542523 1  2.277592\n35 2.045838e+01 9.619522e+00 4.523094e+00 2.126756800 1  9.804237\n25 3.480056e+00 2.547943e+00 1.865491e+00 1.365829700 1 10.988057\n40 2.088532e+01 9.769690e+00 4.570046e+00 2.137766500 1 13.405720\n6  1.103977e+02 3.405806e+01 1.050703e+01 3.241454900 1 14.950901\n1  4.279414e-01 5.291005e-01 6.541723e-01 0.808809170 1 10.073261\n13 1.062850e-02 3.310199e-02 1.030946e-01 0.321083520 1  5.425063\n\n$`5`\n             V1           V2           V3       x_tr b      y_tr\n29 2.086426e-01 3.087111e-01  0.456774142 0.67585068 1  9.142000\n16 5.337705e-07 1.974767e-05  0.000730596 0.02702954 1  1.609707\n3  1.403843e+02 4.078392e+01 11.848389738 3.44214900 1 19.023864\n34 6.213827e+01 2.213194e+01  7.882783409 2.80762950 1 14.299533\n31 3.316681e+01 1.382063e+01  5.759063558 2.39980490 1 11.915797\n20 1.074394e+02 3.337127e+01 10.365298728 3.21951840 1 13.296842\n8  4.896759e+01 1.851107e+01  6.997684571 2.64531370 1 11.362217\n38 5.519668e-03 2.025045e-02  0.074294465 0.27257011 1  5.415621\n```\n:::\n\n```{.r .cell-code}\n# loop for each run\nfor (run in 1:5) {\n    # designate datasets\n    validation_set <- train_groups[[run]]\n    training_set <- do.call(rbind, train_groups[-run])\n    x <- as.matrix(training_set[1:5])\n    y <- as.matrix(training_set[6])\n\n    # compute weight vector\n    w <- solve(t(x) %*% x) %*% (t(x) %*% y)\n\n    # create identity matrix with 0 at position (1, 1)\n    i_hat <- diag(nrow(w))\n    i_hat[1, 1] <- 0\n\n    # function calculating validation error in terms of lambda\n    l2_error <- function(lambda) {\n        l <- 0.5 * norm(\n            as.matrix(validation_set[1:5]) %*% w - as.matrix(validation_set[6])\n        )^2 + lambda / 2 * t(w) %*% i_hat %*% w\n        return(l)\n    }\n\n    # plot data\n    plt <- ggplot() +\n        stat_function(fun = l2_error) +\n        scale_x_log10() +\n        labs(\n            title = paste0(\"Run \", run),\n            x = \"log10 lambda\",\n            y = \"error\",\n        ) +\n        ylim(0, 2000) +\n        theme_light()\n    print(plt)\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n:::\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n:::\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n:::\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n:::\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-9-4.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lambda/2 * t(w) %*% i_hat %*% w: Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n```\n:::\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-9-5.png){width=672}\n:::\n:::\n\nWe can see slight variation in each figure, but the trend remains the same: the error has a positive correlation with lambda, thus minimizing lambda seems to be the best fit for our data. This is the same trend seen previously.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot 4th order polynomial regression on test set\nplt_te_4 <- ggplot(data = df_te, mapping = aes(x = x_te, y = y_te)) +\n    geom_point() +\n    stat_function(fun = function(x) {\n        (w4[1] * x^4) + (w4[2] * x^3) + (w4[3] * x^2) + (w4[4] * x) + w4[5]\n    }) +\n    xlim(0, 4) +\n    ylim(0, 30) +\n    theme_light()\nplt_te_4\n```\n\n::: {.cell-output-display}\n![](linreg_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nThe best fit 4th order polynomial regression plot.\n\n# Addition\nResponse to Question 1d, in Refreshing Mathematics\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rgl)\n\n# Define A and B matrices\nA <- matrix(c(2, -2, -2, 3), nrow = 2)\nB <- matrix(c(2, -1, -1, 2), nrow = 2)\n\n# Define the function to plot\nf <- function(w) {\n  return(sum(diag(B %*% w %*% t(w) %*% A)))\n}\n\n# Generate a grid of (x, y) points\nx <- seq(-1, 1, length.out = 50)\ny <- seq(-1, 1, length.out = 50)\ngrid <- expand.grid(x, y)\n\n# Evaluate the function at each (x, y) point\nz <- apply(grid, 1, function(w) f(as.matrix(w)))\n\n# Create the 3D plot\nz <- matrix(z, nrow = length(x), ncol = length(y), byrow = TRUE)\npersp3d(x, y, z, col = heat.colors(1000))\n```\n:::",
    "supporting": [
      "linreg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}